{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ab424e78ad5f4d02ac95888244a38b9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c7c854521fc4346914fda0d248f78c1",
              "IPY_MODEL_3aa0319d45344611847ed7cb1524cf1c",
              "IPY_MODEL_ec45728b1da4401a8a67d243d484f6f2"
            ],
            "layout": "IPY_MODEL_dd0077c48fb24206b0c9f9465ef6cc95"
          }
        },
        "6c7c854521fc4346914fda0d248f78c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac68d7ac3fa74f2cb48990f2390918d8",
            "placeholder": "​",
            "style": "IPY_MODEL_8ecfc0b96ab6404cb261513c0a149a96",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "3aa0319d45344611847ed7cb1524cf1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c715daac4cc84f8b854e90bc69e2698c",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_adab278583d948698baa45d1e6e66fbe",
            "value": 570
          }
        },
        "ec45728b1da4401a8a67d243d484f6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0be3fa85ae6340a9b90b2e7d3537b9c7",
            "placeholder": "​",
            "style": "IPY_MODEL_99ac19b4269947a4bfb5fa9e02489208",
            "value": " 570/570 [00:00&lt;00:00, 10.3kB/s]"
          }
        },
        "dd0077c48fb24206b0c9f9465ef6cc95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac68d7ac3fa74f2cb48990f2390918d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ecfc0b96ab6404cb261513c0a149a96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c715daac4cc84f8b854e90bc69e2698c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adab278583d948698baa45d1e6e66fbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0be3fa85ae6340a9b90b2e7d3537b9c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99ac19b4269947a4bfb5fa9e02489208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77df8d6ee2974564888e9736b90f27bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0bcce40b73f2422c87d0087d2684d554",
              "IPY_MODEL_a76332a26c094910aa3e8f240982d02c",
              "IPY_MODEL_7206e2f668684cbd8a136a00378a6e7f"
            ],
            "layout": "IPY_MODEL_58ef0deb4047457d9155470bd92a5227"
          }
        },
        "0bcce40b73f2422c87d0087d2684d554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e632f72850b469eace5962b3a53029e",
            "placeholder": "​",
            "style": "IPY_MODEL_3688f45618a24a6993be324fcb8c08d6",
            "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
          }
        },
        "a76332a26c094910aa3e8f240982d02c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2cb1252a5eb42d3b996667c576eb264",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0850eb7a40e488fb1dd8f25702df801",
            "value": 440473133
          }
        },
        "7206e2f668684cbd8a136a00378a6e7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5be4519179048be8bbc3f552415cc76",
            "placeholder": "​",
            "style": "IPY_MODEL_7b13d350d1cd4a4dad9de6a05c26690b",
            "value": " 440M/440M [00:04&lt;00:00, 96.8MB/s]"
          }
        },
        "58ef0deb4047457d9155470bd92a5227": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e632f72850b469eace5962b3a53029e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3688f45618a24a6993be324fcb8c08d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2cb1252a5eb42d3b996667c576eb264": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0850eb7a40e488fb1dd8f25702df801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5be4519179048be8bbc3f552415cc76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b13d350d1cd4a4dad9de6a05c26690b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11b24993561b48df93516f07364d0fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b121e9d72ba45889b381ff3e64153cb",
              "IPY_MODEL_a6f13ed4b9d74bb58e7b0ca98c32671d",
              "IPY_MODEL_0e4b8510cc2549aab0544959a2818d24"
            ],
            "layout": "IPY_MODEL_cec5798bb30f4048b96eb110caf67981"
          }
        },
        "3b121e9d72ba45889b381ff3e64153cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33e6e4a700b74c92aec38fe53e22b009",
            "placeholder": "​",
            "style": "IPY_MODEL_12f922ae84a5494e89d15f0b35afe43e",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "a6f13ed4b9d74bb58e7b0ca98c32671d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_448001628ca54935a82dba87295aa583",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0df5ffaed55e4f358d8ec4af473fee73",
            "value": 28
          }
        },
        "0e4b8510cc2549aab0544959a2818d24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd477e28ffa74380bc99ae0ef565e2a0",
            "placeholder": "​",
            "style": "IPY_MODEL_afb2d5090cbe434f87f814edc49b8f74",
            "value": " 28.0/28.0 [00:00&lt;00:00, 877B/s]"
          }
        },
        "cec5798bb30f4048b96eb110caf67981": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33e6e4a700b74c92aec38fe53e22b009": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12f922ae84a5494e89d15f0b35afe43e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "448001628ca54935a82dba87295aa583": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0df5ffaed55e4f358d8ec4af473fee73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd477e28ffa74380bc99ae0ef565e2a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afb2d5090cbe434f87f814edc49b8f74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afada74e47b145c8bc71cd913e8a352d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb6b92a536a4458799b0ba44aa592e8d",
              "IPY_MODEL_7210d3257e9c4620be55b84e98ee0dce",
              "IPY_MODEL_016cac91894343ccaa084df9b2316fdf"
            ],
            "layout": "IPY_MODEL_208024ead0384a61af5046741b13c0e3"
          }
        },
        "cb6b92a536a4458799b0ba44aa592e8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac9e87737da643b1b5a0b0b71bad8142",
            "placeholder": "​",
            "style": "IPY_MODEL_46e6236bafd241e2b7e0fd5f2aaa0c61",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "7210d3257e9c4620be55b84e98ee0dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c03add368ed430d84dd57652ba4aa13",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45c45dee856a4c2fb7d6a8927b9320b9",
            "value": 231508
          }
        },
        "016cac91894343ccaa084df9b2316fdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d4c6fc3668f49fc9b187487fbe472bd",
            "placeholder": "​",
            "style": "IPY_MODEL_9bb0d66b81b7436f989cdd8aafd8dc20",
            "value": " 232k/232k [00:00&lt;00:00, 2.33MB/s]"
          }
        },
        "208024ead0384a61af5046741b13c0e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac9e87737da643b1b5a0b0b71bad8142": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46e6236bafd241e2b7e0fd5f2aaa0c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c03add368ed430d84dd57652ba4aa13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45c45dee856a4c2fb7d6a8927b9320b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d4c6fc3668f49fc9b187487fbe472bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bb0d66b81b7436f989cdd8aafd8dc20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5410f1e2c66488283241047273c9bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b3d2a71d3df48bda3f4cbc11a299243",
              "IPY_MODEL_9d25a5f1dd504e7a9fb7740d027898a4",
              "IPY_MODEL_f17efe7111804320bbf886109df1200f"
            ],
            "layout": "IPY_MODEL_5cd60ca5aa9d4e39bd633f5fd571069a"
          }
        },
        "2b3d2a71d3df48bda3f4cbc11a299243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_516238cf88ec43aebca866313aa282d9",
            "placeholder": "​",
            "style": "IPY_MODEL_4045dcc3fa2f40d480fb6d7070f99cee",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "9d25a5f1dd504e7a9fb7740d027898a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c8d81f40f224f238112b5dfb2ef444a",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b0c46d1800d4915b1c10cd7e83c6f2e",
            "value": 466062
          }
        },
        "f17efe7111804320bbf886109df1200f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7139386af994153a3356226d3a41c3f",
            "placeholder": "​",
            "style": "IPY_MODEL_6d8901e0158746708161141aba377ca6",
            "value": " 466k/466k [00:00&lt;00:00, 3.92MB/s]"
          }
        },
        "5cd60ca5aa9d4e39bd633f5fd571069a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "516238cf88ec43aebca866313aa282d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4045dcc3fa2f40d480fb6d7070f99cee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c8d81f40f224f238112b5dfb2ef444a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b0c46d1800d4915b1c10cd7e83c6f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7139386af994153a3356226d3a41c3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d8901e0158746708161141aba377ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nana326/INFO-159/blob/main/HW3/HW_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 3: Language Models, Contextual Embedding and BERT\n",
        "\n",
        "In this homework, we will explore implementations of various language models we saw in lecture. We will explore BERT and measure perplexity. "
      ],
      "metadata": {
        "id": "ejZ2oE4GmdfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Set Up\n",
        "\n",
        "If you're opening this Notebook on colab, you will probably need to install Transformers. Make sure your version of Transformers is at least 4.11.0"
      ],
      "metadata": {
        "id": "Dj0EWMnOmgWw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "svbBi3YGmMJX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0fd3545-2eb9-478f-a138-865580483198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "JhnQ3PoVnH7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9195bfe-2929-40ae-c67b-d752e30d103b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANT: For this assignment, GPU is not necessary. The following code block should show \"Running on cpu\". \n",
        "Go to Runtime > Change runtime type > Hardware accelerator > None if otherwise."
      ],
      "metadata": {
        "id": "LpEDSQsLnbLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ],
      "metadata": {
        "id": "r7osYx6Hm0vh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a89a39-9951-4689-d9e7-f834dbd65b80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masking\n",
        "\n",
        "One of the core ideas to wrap your head around with transformer-based language models (and PyTorch) is the concept of *masking*---preventing a model from seeing specific tokens in the input during training.\n",
        "\n",
        "* BERT training relies on the concept of *masked language modeling*: masking a random set of input tokens in a sequence and attempting to predict them.  Remember that BERT is *bidirectional*, so that it can use all of the other non-masked tokens in a sentence to make that prediction.\n",
        "\n",
        "* The GPT class of models acts as a traditional left-to-right language model (sometimes called a \"causal\" LM) .  This family also uses self-attention based transformers---but, when making a prediction for the word $w_i$ at position $i$, it can only use information about words $w_1, \\ldots, w_{i-1}$ to do so.  All of the other tokens following position $i-1$ must be *masked* (hidden from view).\n"
      ],
      "metadata": {
        "id": "KQ4XlcOvIHgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Think about a mask as a matrix that's applied to every input $w$ when generating an output $o$ that determines whether an given $o_i$ is allowed to access each token in $w$.  For example, when passing a three-word input sequence through a transformer (to yield a three-word output sequence), a mask is a $3 \\times 3$ matrix where the cells are essentially  answering the following questions:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "o_1 \\; \\textrm{hide} \\; w_1\\textrm{?} & o_1 \\; \\textrm{hide} \\; w_2\\textrm{?} & o_1 \\; \\textrm{hide} \\; w_3\\textrm{?} \\\\\n",
        "o_2 \\; \\textrm{hide} \\; w_1\\textrm{?} & o_2 \\; \\textrm{hide} \\; w_2\\textrm{?} & o_2 \\; \\textrm{hide} \\; w_3\\textrm{?} \\\\\n",
        "o_3 \\; \\textrm{hide} \\; w_1\\textrm{?} & o_3 \\; \\textrm{hide} \\; w_2\\textrm{?} & o_3 \\; \\textrm{hide} \\; w_3\\textrm{?} \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "In the masks we will consider below, 1 denotes that a position should be hidden; 0 denotes that it should be visible. Consider this mask:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & 1 & 1 \\\\\n",
        "1 & 0 & 1 \\\\\n",
        "1 & 1 & 0\n",
        "\\end{bmatrix}\n",
        "\n",
        "And consider this sequence:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "\\textrm{John} & \\textrm{likes}  & \\textrm{dogs}  \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "When applying this mask to that sequence, we're saying that when we're generating the output for $o_1$ (*John*), we can only consider $w_1$ as an input (*John*).  Likewise, when we generate the output for $o_2$ (*likes*), we can only consider $w_2$ as an input (*likes*), and so on.  (This is a terrible mask!  But illustrates what function a mask performs.)\n",
        "\n",
        "The following code illustrates how this works for that particular mask.\n"
      ],
      "metadata": {
        "id": "Yi0LW-vKZnIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def visualize_masking(sequences, mask):\n",
        "  print(mask)\n",
        "  for sequence in sequences:\n",
        "    for i in range(len(sequence)):\n",
        "      visible=[]\n",
        "      for j in range(len(sequence)):\n",
        "        if mask[i][j]==0:\n",
        "          visible.append(sequence[j])\n",
        "      print(\"for word %s, the following tokens are visible: %s\" % (sequence[i], visible))\n",
        "    print()"
      ],
      "metadata": {
        "id": "Ea2E6zlMZkzO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences=[[\"This\", \"is\", \"a\", \"sentence\", \"that\", \"has\", \"exactly\", \"ten\", \"tokens\", \".\"], [\"Here's\", \"another\", \"sequence\", \"with\", \"10\", \"words\", \"like\", \"the\", \"last\", \".\"]]\t\n",
        "\n",
        "seq_length=len(sequences[0])\n",
        "\n",
        "test_mask=np.ones((seq_length,seq_length))\n",
        "for i in range(seq_length):\n",
        "  test_mask[i,i]=0\n",
        "\n",
        "visualize_masking(sequences, test_mask)\n",
        "\n"
      ],
      "metadata": {
        "id": "xROu5KmvKMua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab6613da-de93-492c-f16f-7dc59f670932"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]]\n",
            "for word This, the following tokens are visible: ['This']\n",
            "for word is, the following tokens are visible: ['is']\n",
            "for word a, the following tokens are visible: ['a']\n",
            "for word sentence, the following tokens are visible: ['sentence']\n",
            "for word that, the following tokens are visible: ['that']\n",
            "for word has, the following tokens are visible: ['has']\n",
            "for word exactly, the following tokens are visible: ['exactly']\n",
            "for word ten, the following tokens are visible: ['ten']\n",
            "for word tokens, the following tokens are visible: ['tokens']\n",
            "for word ., the following tokens are visible: ['.']\n",
            "\n",
            "for word Here's, the following tokens are visible: [\"Here's\"]\n",
            "for word another, the following tokens are visible: ['another']\n",
            "for word sequence, the following tokens are visible: ['sequence']\n",
            "for word with, the following tokens are visible: ['with']\n",
            "for word 10, the following tokens are visible: ['10']\n",
            "for word words, the following tokens are visible: ['words']\n",
            "for word like, the following tokens are visible: ['like']\n",
            "for word the, the following tokens are visible: ['the']\n",
            "for word last, the following tokens are visible: ['last']\n",
            "for word ., the following tokens are visible: ['.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q1.  \n",
        "As we discussed in class, BERT masks a random set of words in the input and attempts to reconstruct those words as output.  Create a mask that randomly masks token positions 2 and 7 (for an input sequence length of 10 tokens, with 0 being the position of the first token).  For an input sequence of 10 tokens, you should generate output representations for all 10 tokens (i.e., $[o_1, \\ldots, o_{10}]$ in the notation above, but each representation must ignore the same 2 input tokens."
      ],
      "metadata": {
        "id": "LKv3h625eMo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bert_mask(seq_length):\n",
        "  mask=np.ones((seq_length,seq_length))\n",
        "  # implement BERT mask here\n",
        "  \n",
        "  # BEGIN SOLUTION\n",
        "\n",
        "  for i in range(len(mask)):\n",
        "    for j in range(len(mask)):\n",
        "      mask[i][j] = 0\n",
        "      if j == 2 or j == 7:\n",
        "        mask[i][j] = 1\n",
        "    # mask[i][2] = 0\n",
        "    # mask[i][7] = 0\n",
        "\n",
        "  # END SOLUTION\n",
        "\n",
        "  return mask"
      ],
      "metadata": {
        "id": "_31mkhIke-YI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2\n",
        "A left-to-right language model (such as GPT) can only use information from input words $[w_1, \\ldots, w_{i}]$ when generating the representation for output $o_i$.  Encode this as a mask as well."
      ],
      "metadata": {
        "id": "ZKjasreUfLww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_causal_mask(seq_length):\n",
        "  mask=np.ones((seq_length,seq_length))\n",
        "  # implement causal mask here\n",
        "\n",
        "  # BEGIN SOLUTION\n",
        "\n",
        "  for i in range(len(mask)):\n",
        "    for j in range(len(mask)):\n",
        "      if i >= j:\n",
        "        mask[i][j] = 0\n",
        "\n",
        "  # END SOLUTION\n",
        "  \n",
        "  return mask"
      ],
      "metadata": {
        "id": "svKuQyQff4fK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's go ahead and embed these masks within a model.  First, we'll load some textual data (from Austen's *Pride and Prejudice*)."
      ],
      "metadata": {
        "id": "tXRkI2DZnwvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.gutenberg.org/files/1342/1342-0.txt"
      ],
      "metadata": {
        "id": "cgTwRcqDn6ni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45725fa9-be68-4172-f3ae-fee59f7fe813"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 03:33:45--  https://www.gutenberg.org/files/1342/1342-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 772145 (754K) [text/plain]\n",
            "Saving to: ‘1342-0.txt.1’\n",
            "\n",
            "1342-0.txt.1        100%[===================>] 754.05K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-02-22 03:33:45 (5.27 MB/s) - ‘1342-0.txt.1’ saved [772145/772145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "7v2EJeEBobD8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "u8hjeySdojyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7dcdaae-a4c5-4ee9-9aaf-e9be01c19f53"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's read in the data and tokenize it; for this homework, we'll only work with the first 10,000 tokens of that book; we'll keep only the most frequent 1,000 word types (all other tokens will be mapped to an [UNK] token)."
      ],
      "metadata": {
        "id": "jre3sSWgvnwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(filename):\n",
        "  with open(filename) as file:\n",
        "    data=file.read().lower()\n",
        "    first10K=' '.join(data.split(\" \")[:10000])\n",
        "    toks=nltk.word_tokenize(first10K)[:10000]\n",
        "    vocab={\"[PAD]\":0, \"[UNK]\":1}\n",
        "    counts=Counter()\n",
        "    for tok in toks:\n",
        "      counts[tok]+=1\n",
        "    for v, _ in counts.most_common(1000):\n",
        "      vocab[v]=len(vocab)\n",
        "    tokids=[]\n",
        "    for tok in toks:\n",
        "      tokid=1\n",
        "      if tok in vocab:\n",
        "        tokid=vocab[tok]\n",
        "      \n",
        "      tokids.append(tokid)\n",
        "\n",
        "    return tokids, vocab  "
      ],
      "metadata": {
        "id": "7kzw9IcFoDZq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's specify our model in PyTorch."
      ],
      "metadata": {
        "id": "4r_XHBZPv4kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "class MaskedLM(nn.Module):\n",
        "    def __init__(self, vocab, mask, d_model=512):       \n",
        "        super().__init__()\n",
        "        self.vocab=vocab\n",
        "        self.mask=mask\n",
        "        vocab_size=len(vocab)\n",
        "        self.embeddings=nn.Embedding(1002,512)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead=8, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "        self.linear=torch.nn.Linear(d_model, vocab_size)\n",
        "        self.rev_vocab={vocab[k]:k for k in vocab}\n",
        "\n",
        "    def forward(self, input): \n",
        "        # first we pass the input word IDS through an embedding layer to get embeddings for them\n",
        "        input=self.embeddings(input)\n",
        "        # then we pass those embeddings through a transformer to get contextual representations, masking the input where appropriate\n",
        "        out = self.transformer_encoder.forward(input, mask=self.mask)        \n",
        "        # finally we pass those embeddings through a linear layer to transform it into the output space (the size of our vocabulary)\n",
        "        h=self.linear(out)\n",
        "        return h"
      ],
      "metadata": {
        "id": "p74L0s1DsKa2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(xs, ys, batch_size=32):\n",
        "    batch_x=[]\n",
        "    batch_y=[]\n",
        "    for i in range(0, len(xs), batch_size):\n",
        "        batch_x.append(torch.LongTensor(xs[i:i+batch_size]).to(device))\n",
        "        batch_y.append(torch.LongTensor(ys[i:i+batch_size]).to(device))\n",
        "    return batch_x, batch_y"
      ],
      "metadata": {
        "id": "njdPnGVQsQOl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokids, vocab=read_data(\"1342-0.txt\")  "
      ],
      "metadata": {
        "id": "xm6tLzHmtAfN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(mask, data_function, tokids, vocab):\n",
        "\n",
        "    mask=torch.BoolTensor(mask).to(device)\n",
        "\n",
        "    num_labels=len(vocab)\n",
        "    model=MaskedLM(vocab, mask).to(device)\n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "    cross_entropy=nn.CrossEntropyLoss()\n",
        "    losses=[]\n",
        "\n",
        "    xs, ys=data_function(tokids)\n",
        "\n",
        "    batch_x, batch_y=get_batches(xs, ys)\n",
        "\n",
        "    for epoch in range(1):\n",
        "        model.train()\n",
        "        \n",
        "        for x, y in list(zip(batch_x, batch_y)):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred=model.forward(x)\n",
        "            loss=cross_entropy(y_pred.view(-1, num_labels), y.view(-1))\n",
        "            losses.append(loss.item())\n",
        "            print(loss)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ],
      "metadata": {
        "id": "9822mHI3sR1Q"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model and training process are now all defined; all that remains is to pass our inputs and outputs through it to train.  Your job here is to create the correct inputs (x) and outputs (y) to train a left-to-right (causal) language model.\n",
        "\n",
        "##Q3\n",
        "Write a function that takes in a sequence of token ids $[w_1, \\ldots, w_n]$ and segments it into 8-token chunks -- e.g., $x_1=[w_1, \\ldots, w_8]$, $x_2=[w_9, \\ldots, w_{16}]$, etc.  For each $x_i$, also create its corresponding $y_i$.  Given this language modeling specification, each $y_i$ should also contain 8 values (for each token in $x_i$).  Keep in mind this is a left-to-right causal language model; your job is to figure out the values of y that respects this design.  At token position $i$, when a model has access to $[w_1, \\ldots, w_i]$, which is the true $y_i$ for that position? Each element in $y$ should be a word ID (i.e., an integer)."
      ],
      "metadata": {
        "id": "1CXKNwLBwBG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_causal_xy(data, max_len=8):\n",
        "    xs=[]\n",
        "    ys=[]\n",
        "    \n",
        "    # BEGIN SOLUTION\n",
        "\n",
        "    for i in range(0, len(data), max_len):\n",
        "      if (len(data[i:i+max_len]) == max_len):\n",
        "        xs.append(data[i:i+max_len])\n",
        "        ys.append(data[i+1:i+max_len+1])\n",
        "    # for j in range(0, len(xs)):\n",
        "    #   ys.append(create_causal_mask(len(xs[j])))\n",
        "\n",
        "    # END SOLUTION\n",
        "\n",
        "    return xs, ys"
      ],
      "metadata": {
        "id": "JIO8M9AeszAi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length=8\n",
        "\n",
        "train(create_causal_mask(seq_length=seq_length), get_causal_xy, tokids, vocab)"
      ],
      "metadata": {
        "id": "AO4WOusZsWsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c59300-9241-4a9b-c12c-9a0a37542c56"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(7.0044, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.4486, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3523, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.9989, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.2021, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.0381, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.9862, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.0856, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.1817, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.5341, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3024, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.6288, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.3831, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1887, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1044, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.0457, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.9401, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.8182, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.7237, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.2831, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.2181, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1048, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.5081, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.9434, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q4 (Write-up)  \n",
        "In this model, as implemented, does the following equivalence hold?\n",
        "\n",
        "$$\n",
        "P(y_4 \\mid w_1 = \\textrm{go}, w_2=\\textrm{ahead}, w_3=\\textrm{make}, w_4=\\textrm{my})= P(y_4 \\mid w_1 = \\textrm{ahead}, w_2=\\textrm{my}, w_3=\\textrm{make}, w_4=\\textrm{go})\n",
        "$$\n",
        "\n",
        "Why or why not?"
      ],
      "metadata": {
        "id": "R2P8GU1VsKLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perplexity\n",
        "To evaluate how good our language model is, we use a metric called perplexity. The perplexity of a language model (PP) on a test set is the inverse probability of the test set, normalized by the number of words. Let $W = w_{1}w_{2}\\dots w_{N}$. Then,\n",
        "\n",
        "$$PP(W) = \\sqrt[N]{\\prod_{i = 1}^{N}\\frac{1}{P(w_{i}|w_{1}\\dots w_{i - 1})}}$$\n",
        "\n",
        "However, since these probabilities are often small, taking the inverse and multiplying can be numerically unstable, so we often first compute these values in the log domain and then convert back. So this equation looks like:\n",
        "\n",
        "$$\\ln PP(W) = \\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i}|w_{1}\\dots w_{i - 1})$$\n",
        "\n",
        "$$\\implies PP(W) = e^{\\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i}|w_{1}\\dots w_{i - 1})}$$\n",
        "\n",
        "Here we want to calculate the perplexity of [pretrained BERT model](https://huggingface.co/bert-base-uncased) on text from different sources. When calculating perplexity with BERT, we'll use a related measure of pseudo-perplexity, which allow us to condition on the bidirectional context (and not just the left context, as in standard perplexity):\n",
        "\n",
        "$$PP(W) = e^{\\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i} \\mid w_{1}\\dots w_{i - 1}, w_{i+1}, \\ldots, w_n)}$$\n"
      ],
      "metadata": {
        "id": "-pZpVrqchSAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's instantiate a BERT model, along with its WordPiece tokenizer."
      ],
      "metadata": {
        "id": "m2MXSfzlsHp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model=model.to(device)"
      ],
      "metadata": {
        "id": "XxyJaS1-cP3R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249,
          "referenced_widgets": [
            "ab424e78ad5f4d02ac95888244a38b9b",
            "6c7c854521fc4346914fda0d248f78c1",
            "3aa0319d45344611847ed7cb1524cf1c",
            "ec45728b1da4401a8a67d243d484f6f2",
            "dd0077c48fb24206b0c9f9465ef6cc95",
            "ac68d7ac3fa74f2cb48990f2390918d8",
            "8ecfc0b96ab6404cb261513c0a149a96",
            "c715daac4cc84f8b854e90bc69e2698c",
            "adab278583d948698baa45d1e6e66fbe",
            "0be3fa85ae6340a9b90b2e7d3537b9c7",
            "99ac19b4269947a4bfb5fa9e02489208",
            "77df8d6ee2974564888e9736b90f27bd",
            "0bcce40b73f2422c87d0087d2684d554",
            "a76332a26c094910aa3e8f240982d02c",
            "7206e2f668684cbd8a136a00378a6e7f",
            "58ef0deb4047457d9155470bd92a5227",
            "7e632f72850b469eace5962b3a53029e",
            "3688f45618a24a6993be324fcb8c08d6",
            "b2cb1252a5eb42d3b996667c576eb264",
            "c0850eb7a40e488fb1dd8f25702df801",
            "a5be4519179048be8bbc3f552415cc76",
            "7b13d350d1cd4a4dad9de6a05c26690b",
            "11b24993561b48df93516f07364d0fc9",
            "3b121e9d72ba45889b381ff3e64153cb",
            "a6f13ed4b9d74bb58e7b0ca98c32671d",
            "0e4b8510cc2549aab0544959a2818d24",
            "cec5798bb30f4048b96eb110caf67981",
            "33e6e4a700b74c92aec38fe53e22b009",
            "12f922ae84a5494e89d15f0b35afe43e",
            "448001628ca54935a82dba87295aa583",
            "0df5ffaed55e4f358d8ec4af473fee73",
            "cd477e28ffa74380bc99ae0ef565e2a0",
            "afb2d5090cbe434f87f814edc49b8f74",
            "afada74e47b145c8bc71cd913e8a352d",
            "cb6b92a536a4458799b0ba44aa592e8d",
            "7210d3257e9c4620be55b84e98ee0dce",
            "016cac91894343ccaa084df9b2316fdf",
            "208024ead0384a61af5046741b13c0e3",
            "ac9e87737da643b1b5a0b0b71bad8142",
            "46e6236bafd241e2b7e0fd5f2aaa0c61",
            "2c03add368ed430d84dd57652ba4aa13",
            "45c45dee856a4c2fb7d6a8927b9320b9",
            "8d4c6fc3668f49fc9b187487fbe472bd",
            "9bb0d66b81b7436f989cdd8aafd8dc20",
            "e5410f1e2c66488283241047273c9bb6",
            "2b3d2a71d3df48bda3f4cbc11a299243",
            "9d25a5f1dd504e7a9fb7740d027898a4",
            "f17efe7111804320bbf886109df1200f",
            "5cd60ca5aa9d4e39bd633f5fd571069a",
            "516238cf88ec43aebca866313aa282d9",
            "4045dcc3fa2f40d480fb6d7070f99cee",
            "3c8d81f40f224f238112b5dfb2ef444a",
            "2b0c46d1800d4915b1c10cd7e83c6f2e",
            "b7139386af994153a3356226d3a41c3f",
            "6d8901e0158746708161141aba377ca6"
          ]
        },
        "outputId": "8524f539-32ff-4c3e-c207-bd1caa83f717"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab424e78ad5f4d02ac95888244a38b9b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77df8d6ee2974564888e9736b90f27bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11b24993561b48df93516f07364d0fc9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afada74e47b145c8bc71cd913e8a352d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5410f1e2c66488283241047273c9bb6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the BERT tokenizer tokenizes a sentence into a sequence of WordPiece ids.  Note how BERT tokenization automatically wraps an input sentences with [CLS] and [SEP] tags."
      ],
      "metadata": {
        "id": "bcLAWTU8sTqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"A dog landed on Mars\"\n",
        "tensor_input = tokenizer(sentence, return_tensors=\"pt\")\n",
        "print(tensor_input)\n",
        "tensor_input_ids = tensor_input[\"input_ids\"]\n",
        "print(tensor_input_ids)\n",
        "print(tokenizer.convert_ids_to_tokens(tensor_input_ids[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxlOsB0NshJm",
        "outputId": "6f18641f-fb60-4b62-cdce-c4e965e79585"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101, 1037, 3899, 5565, 2006, 7733,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "tensor([[ 101, 1037, 3899, 5565, 2006, 7733,  102]])\n",
            "['[CLS]', 'a', 'dog', 'landed', 'on', 'mars', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see how we can calculate output probabilities using this model.  The output of each token position $i$ gives us $P(w_i \\mid w_1, \\ldots, w_n)$---the probability of the word at that position over our vocabulary, given *all* of the words in the sentence."
      ],
      "metadata": {
        "id": "oGu38_DB1vsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  output = model(tensor_input_ids)\n",
        "  logits = output.logits\n",
        "  # logits here are the unnormalized scores, so let's pass them through the softmax \n",
        "  # to get a probability distribution\n",
        "  softmax = torch.nn.functional.softmax(logits, dim = -1)\n",
        "  # for one input sequence, the shape of the resulting distribution is: \n",
        "  # 1 x [length of input, in WordPiece tokens] x (the size of the BERT vocabulary)\n",
        "  print(softmax.shape) # [1, 7, 30522]\n",
        "  input_ints=tensor_input_ids.numpy()[0]\n",
        "  # Let's print the probability of the true inputs\n",
        "  wp_tokens=tokenizer.convert_ids_to_tokens(input_ints)\n",
        "  for i in range(len(input_ints)):\n",
        "    prob=softmax[0][i][input_ints[i]].numpy()\n",
        "    print(\"%s\\t%s\\t%.5f\" % (wp_tokens[i], input_ints[i], prob))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvuNw52m15OU",
        "outputId": "c31e7a64-cf65-4665-b4cb-709fe745bac8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 7, 30522])\n",
            "[CLS]\t101\t0.00000\n",
            "a\t1037\t0.99281\n",
            "dog\t3899\t0.99052\n",
            "landed\t5565\t0.99809\n",
            "on\t2006\t0.99874\n",
            "mars\t7733\t0.00133\n",
            "[SEP]\t102\t0.00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that $w_i$ is in the range $[w_1, \\ldots, w_n]$ -- clearly the probability of a word is going to be high when we can observe it in the input! Let's do some masking to calculate $P(w_i \\mid w_1, \\ldots w_{i-1}, w_{i+1}, w_n)$.  Now annoyingly, BERT's `attention_mask` function only works for padding tokens; to mask input tokens, we need to intervene in the input and replace a WordPiece token that we're predicting with a special [MASK] token (BERT tokenizer word id `103`)."
      ],
      "metadata": {
        "id": "F90uSrL_1vot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "with torch.no_grad():\n",
        "  # let's make a copy of the original word ids so we can mask one of the tokens\n",
        "  masked_input_ids=copy.deepcopy(tensor_input_ids)\n",
        "  # we'll mask the second word\n",
        "  masked_input_ids[0][1]=tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
        "\n",
        "  print(\"The second word here now is [MASK] token ID '103': \", masked_input_ids)\n",
        "\n",
        "  # now let's run that through BERT in the same way we did before\n",
        "  output = model(masked_input_ids)\n",
        "  logits = output.logits\n",
        "\n",
        "  softmax = torch.nn.functional.softmax(logits, dim = -1)\n",
        "  input_ints=tensor_input_ids.numpy()[0]\n",
        "\n",
        "  wp_tokens=tokenizer.convert_ids_to_tokens(input_ints)\n",
        "  i=1\n",
        "  prob=softmax[0][i][input_ints[i]].numpy()\n",
        "  print(\"%s\\t%s\\t%.5f\" % (wp_tokens[i], input_ints[i], prob))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77EdDCtk3MSa",
        "outputId": "c50f50b2-bdfa-43bd-de38-8511823c4e11"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The second word here now is [MASK] token ID '103':  tensor([[ 101,  103, 3899, 5565, 2006, 7733,  102]])\n",
            "a\t1037\t0.13965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see the probability of \"a\" as the second token has gone down to 0.13965 when we mask it.  This is the $P(w_1 =\\textrm{a} \\mid w_0, w_2, \\ldots, w_n)$.  At this point you should have everything you need to calculate the BERT pseudo-perplexity of an input sentence."
      ],
      "metadata": {
        "id": "PSdj7onm1vln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q5\n",
        "Implement the pseudo-perplexity measure described above, calculating the perplexity for a given model, tokenizer, and sentence. \n",
        "\n",
        "The function calculates the average probability of each token in the sentence given all the other tokens. We need to predict the probability of each word in a sentence by masking the one word to predict. Note that you should not include the probabilities of the [CLS] and [SEP] tokens in your perplexity equation -- those tokens are not part of the original test sentence.\n",
        "\n"
      ],
      "metadata": {
        "id": "vA_eHLic_OfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function calculates the perplexity of a language model, given a sentence and its corresponding tokenizer\n",
        "\n",
        "# Inputs:\n",
        "# model: language model being used to calculate the perplexity\n",
        "# tokenizer: tokenizer that is used to preprocess the input sentence\n",
        "# sentence: input sentence string for which perplexity is to be calculated\n",
        "\n",
        "# Outputs:\n",
        "# returns perplexity of the input sentence\n",
        "\n",
        "def perplexity(model, tokenizer, sentence):\n",
        "\n",
        "    # hints: you'll need to:\n",
        "    # encode the input sentence using the tokenizer\n",
        "    # for each WordPiece token in the sentence (except [CLS] and [SEP]), mask that single token and \n",
        "    # calculate the probability of that true word at the masked position\n",
        "    # don't calculate perplexity for the [CLS] and [SEP] tokens (which are not part of the original test sentence).\n",
        "\n",
        "    perplexity=None\n",
        "    # BEGIN SOLUTION\n",
        "\n",
        "    with torch.no_grad():\n",
        "      input = tokenizer(sentence, return_tensors=\"pt\")\n",
        "      input_ids = input[\"input_ids\"]\n",
        "      masked_input_ids=copy.deepcopy(input_ids)\n",
        "\n",
        "      for w in range(1, len(masked_input_ids[0][1:len(masked_input_ids[0])])):\n",
        "        masked_input_ids[0][w]=tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
        "        output = model(masked_input_ids)\n",
        "        logits = output.logits\n",
        "\n",
        "        softmax = torch.nn.functional.softmax(logits, dim = -1)\n",
        "        input_ints=input_ids.numpy()[0]\n",
        "        \n",
        "        wp_tokens=tokenizer.convert_ids_to_tokens(input_ints)\n",
        "        prob=softmax[0][w][input_ints[w]].numpy()\n",
        "        print(\"%s\\t%s\\t%.5f\" % (wp_tokens[w], input_ints[w], prob))\n",
        "      \n",
        "      perplexity = np.exp(np.average(-np.log(prob)))\n",
        "\n",
        "    # END SOLUTION\n",
        "\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "GDZ9Qe8L-uUt"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(perplexity(sentence='London is the capital of the United Kingdom.', model=model, tokenizer=tokenizer))"
      ],
      "metadata": {
        "id": "tN7HVo_oIWXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5a52fc-2e06-4519-9142-ae84357262d5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "london\t2414\t0.64463\n",
            "is\t2003\t0.96583\n",
            "the\t1996\t0.97224\n",
            "capital\t3007\t0.01300\n",
            "of\t1997\t0.01644\n",
            "the\t1996\t0.88721\n",
            "united\t2142\t0.20757\n",
            "kingdom\t2983\t0.00001\n",
            ".\t1012\t0.96486\n",
            "1.036415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# No credit.  "
      ],
      "metadata": {
        "id": "LsHLgqH0-6_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide [texts](https://people.ischool.berkeley.edu/~dbamman/text_from_different_sources.txt) from 4 different sources ([Wikipedia](https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots), [Yelp](https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset), [Fiction](https://github.com/dbamman/litbank), [Twitter](https://github.com/dbamman/anlp21/blob/main/data/potus_tweets.json)) collected from open-source datasets. Each category has 125 entries."
      ],
      "metadata": {
        "id": "i5eR8EF12v1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://people.ischool.berkeley.edu/~dbamman/text_from_different_sources.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnkdWOu1BGjP",
        "outputId": "60ae6e78-8123-46cb-a98a-e13e558e6229"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 03:47:42--  https://people.ischool.berkeley.edu/~dbamman/text_from_different_sources.txt\n",
            "Resolving people.ischool.berkeley.edu (people.ischool.berkeley.edu)... 128.32.78.16\n",
            "Connecting to people.ischool.berkeley.edu (people.ischool.berkeley.edu)|128.32.78.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 61117 (60K) [text/plain]\n",
            "Saving to: ‘text_from_different_sources.txt’\n",
            "\n",
            "text_from_different 100%[===================>]  59.68K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-02-22 03:47:43 (416 KB/s) - ‘text_from_different_sources.txt’ saved [61117/61117]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_by_genre={}\n",
        "with open('text_from_different_sources.txt') as file:\n",
        "  file.readline()\n",
        "  for line in file:\n",
        "    cols=line.rstrip().split(\"\\t\")\n",
        "    genre=cols[0]\n",
        "    text=cols[1]\n",
        "\n",
        "    if genre not in text_by_genre:\n",
        "      text_by_genre[genre]=[]\n",
        "    text_by_genre[genre].append(text)\n",
        "\n",
        "for genre in text_by_genre:\n",
        "  print(genre, len(text_by_genre[genre]))"
      ],
      "metadata": {
        "id": "SpCpGQpG4OJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13bde29b-fa7b-49df-dfa6-5bf2dfb84066"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wikipedia 125\n",
            "Yelp 125\n",
            "Fiction 125\n",
            "Twitter 125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate perplexity on each genre over all of the words present within it; each line contains exactly one sentence for each genre.\n",
        "\n",
        "The output perplexity_by_genre = {} is a dictionary mapping genre to a list of perplexities for each sentence in that genre. For computational purpose, we only take the first 25 sentences as an example (still this can take up to 10 minutes to run), feel free to change 25 to smaller numbers.\n",
        "\n",
        "e.g. perplexity_by_genre['Wikipedia'] should be a list of 25 perplexities (one for each Wikipedia row in the input file)."
      ],
      "metadata": {
        "id": "jtE7Umzy9xRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def calculate_perplexity_by_genre(text_by_genre):\n",
        "  perplexity_by_genre = {}\n",
        "  for genre in text_by_genre:\n",
        "    perplexity_by_genre[genre] = []\n",
        "    for text in text_by_genre[genre][:25]: # change 25 to smaller numbers if necessary\n",
        "      p = perplexity(sentence=text, model=model, tokenizer=tokenizer)\n",
        "      perplexity_by_genre[genre].append(p)\n",
        "  return perplexity_by_genre"
      ],
      "metadata": {
        "id": "09NqLCO0523E"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# running this might take up to 10 minutes\n",
        "perplexity_by_genre = calculate_perplexity_by_genre(text_by_genre)\n",
        "for genre in perplexity_by_genre:\n",
        "  print(\"Genre:\",genre,\", mean perplexity:\",np.mean(perplexity_by_genre[genre]))"
      ],
      "metadata": {
        "id": "7sXsSGbd730C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f7e1b19-b797-4477-9318-583465d12068"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in\t1999\t0.98727\n",
            "the\t1996\t0.89004\n",
            "film\t2143\t0.00557\n",
            ",\t1010\t0.02604\n",
            "lillian\t19344\t0.00367\n",
            "travers\t29053\t0.00009\n",
            ",\t1010\t0.98825\n",
            "a\t1037\t0.98437\n",
            "wealthy\t7272\t0.01314\n",
            "northern\t2642\t0.00028\n",
            "woman\t2450\t0.00001\n",
            "about\t2055\t0.07356\n",
            "to\t2000\t0.60629\n",
            "be\t2022\t0.00049\n",
            "married\t2496\t0.00002\n",
            ",\t1010\t0.01360\n",
            "takes\t3138\t0.00161\n",
            "a\t1037\t0.51372\n",
            "magical\t8687\t0.00104\n",
            "seed\t6534\t0.00002\n",
            "which\t2029\t0.20076\n",
            "transforms\t21743\t0.07225\n",
            "its\t2049\t0.00433\n",
            "user\t5310\t0.00000\n",
            "into\t2046\t0.00413\n",
            "the\t1996\t0.88742\n",
            "opposite\t4500\t0.01511\n",
            "gender\t5907\t0.00002\n",
            ".\t1012\t0.71784\n",
            "mabel\t19486\t0.00089\n",
            "and\t1998\t0.99760\n",
            "her\t2014\t0.00000\n",
            "beau\t17935\t0.00003\n",
            "go\t2175\t0.01941\n",
            "to\t2000\t0.27182\n",
            "an\t2019\t0.39089\n",
            "auto\t8285\t0.00020\n",
            "race\t2679\t0.00030\n",
            "and\t1998\t0.19088\n",
            "are\t2024\t0.48551\n",
            "joined\t2587\t0.03955\n",
            "by\t2011\t0.00675\n",
            "charlie\t4918\t0.00071\n",
            "and\t1998\t0.02736\n",
            "his\t2010\t0.08168\n",
            "friend\t2767\t0.00016\n",
            ".\t1012\t0.88054\n",
            "in\t1999\t0.97089\n",
            "one\t2028\t0.90971\n",
            "of\t1997\t0.00304\n",
            "chaplin\t23331\t0.00755\n",
            "'\t1005\t0.99946\n",
            "s\t1055\t0.00056\n",
            "\"\t1000\t0.00125\n",
            "park\t2380\t0.00003\n",
            "comedies\t22092\t0.00007\n",
            "\"\t1000\t0.00015\n",
            "for\t2005\t0.00094\n",
            "keystone\t22271\t0.00003\n",
            "studios\t4835\t0.00008\n",
            ",\t1010\t0.01314\n",
            "charlie\t4918\t0.00009\n",
            "and\t1998\t0.94363\n",
            "his\t2010\t0.03958\n",
            "dom\t14383\t0.99918\n",
            "##ine\t3170\t0.00120\n",
            "##ering\t7999\t0.00000\n",
            "wife\t2564\t0.01148\n",
            ",\t1010\t0.05659\n",
            "mrs\t3680\t0.00781\n",
            ".\t1012\t0.00833\n",
            "sniff\t27907\t0.01716\n",
            "##les\t4244\t0.00000\n",
            ",\t1010\t0.00468\n",
            "are\t2024\t0.03377\n",
            "walking\t3788\t0.00146\n",
            "in\t1999\t0.17666\n",
            "the\t1996\t0.00254\n",
            "greens\t15505\t0.00016\n",
            "##ward\t7652\t0.00002\n",
            ".\t1012\t0.67837\n",
            "james\t2508\t0.01656\n",
            "birch\t16421\t0.00028\n",
            ",\t1010\t0.99911\n",
            "an\t2019\t0.95498\n",
            "english\t2394\t0.00027\n",
            "hunter\t4477\t0.01375\n",
            ",\t1010\t0.03818\n",
            "is\t2003\t0.88606\n",
            "accidentally\t9554\t0.00120\n",
            "shot\t2915\t0.00424\n",
            "by\t2011\t0.00020\n",
            "the\t1996\t0.42491\n",
            "servant\t7947\t0.00041\n",
            "of\t1997\t0.00233\n",
            "kirk\t11332\t0.00099\n",
            "##e\t2063\t0.00163\n",
            "warren\t6031\t0.00002\n",
            ",\t1010\t0.10998\n",
            "a\t1037\t0.87033\n",
            "wild\t3748\t0.00196\n",
            "animal\t4111\t0.00003\n",
            "painter\t5276\t0.00186\n",
            "who\t2040\t0.00641\n",
            "is\t2003\t0.00607\n",
            "camping\t13215\t0.00006\n",
            "in\t1999\t0.06797\n",
            "the\t1996\t0.98768\n",
            "jungle\t8894\t0.00001\n",
            ".\t1012\t0.71784\n",
            "king\t2332\t0.09311\n",
            "k\t1047\t0.45950\n",
            "##rew\t15603\t0.00001\n",
            "##l\t2140\t0.00713\n",
            "(\t1006\t0.88969\n",
            "raymond\t7638\t0.00012\n",
            "russell\t5735\t0.00001\n",
            ")\t1007\t0.24775\n",
            "is\t2003\t0.19383\n",
            "a\t1037\t0.40088\n",
            "cruel\t10311\t0.00041\n",
            "dictator\t21237\t0.00000\n",
            "in\t1999\t0.16959\n",
            "the\t1996\t0.36100\n",
            "emerald\t14110\t0.00554\n",
            "city\t2103\t0.00283\n",
            "in\t1999\t0.33052\n",
            "the\t1996\t0.91979\n",
            "land\t2455\t0.00214\n",
            "of\t1997\t0.04163\n",
            "oz\t11472\t0.00000\n",
            ".\t1012\t0.84439\n",
            "charlie\t4918\t0.00368\n",
            "and\t1998\t0.99860\n",
            "his\t2010\t0.51991\n",
            "friend\t2767\t0.00002\n",
            "ambrose\t15675\t0.00013\n",
            "meet\t3113\t0.15436\n",
            "in\t1999\t0.30453\n",
            "a\t1037\t0.17248\n",
            "restaurant\t4825\t0.00007\n",
            "and\t1998\t0.01741\n",
            "accidentally\t9554\t0.00007\n",
            "leave\t2681\t0.00007\n",
            "with\t2007\t0.01298\n",
            "each\t2169\t0.97161\n",
            "other\t2060\t0.00246\n",
            "'\t1005\t0.99706\n",
            "s\t1055\t0.00005\n",
            "coats\t15695\t0.00007\n",
            ".\t1012\t0.86708\n",
            "john\t2198\t0.01202\n",
            "howard\t4922\t0.00036\n",
            "payne\t13470\t0.00003\n",
            "leaves\t3727\t0.00640\n",
            "home\t2188\t0.00270\n",
            "and\t1998\t0.11481\n",
            "begins\t4269\t0.00028\n",
            "a\t1037\t0.08833\n",
            "career\t2476\t0.00041\n",
            "in\t1999\t0.32529\n",
            "the\t1996\t0.01092\n",
            "theater\t4258\t0.00001\n",
            ".\t1012\t0.92896\n",
            "im\t10047\t0.00784\n",
            "##ar\t2906\t0.00005\n",
            "the\t1996\t0.22043\n",
            "ser\t14262\t0.88669\n",
            "##vi\t5737\t0.00054\n",
            "##tor\t4263\t0.00005\n",
            "rescues\t26001\t0.00006\n",
            "an\t2019\t0.24376\n",
            "american\t2137\t0.03091\n",
            "tourist\t7538\t0.00217\n",
            "who\t2040\t0.00285\n",
            "has\t2038\t0.11472\n",
            "lost\t2439\t0.03627\n",
            "his\t2010\t0.24580\n",
            "way\t2126\t0.00006\n",
            "in\t1999\t0.33861\n",
            "the\t1996\t0.97008\n",
            "desert\t5532\t0.00002\n",
            "and\t1998\t0.88186\n",
            "the\t1996\t0.99417\n",
            "two\t2048\t0.00431\n",
            "men\t2273\t0.00001\n",
            "become\t2468\t0.01285\n",
            "friends\t2814\t0.00062\n",
            ".\t1012\t0.82102\n",
            "the\t1996\t0.99766\n",
            "following\t2206\t0.84895\n",
            "plot\t5436\t0.38957\n",
            "syn\t19962\t0.99981\n",
            "##opsis\t22599\t0.00029\n",
            "was\t2001\t0.90590\n",
            "published\t2405\t0.02159\n",
            "in\t1999\t0.99646\n",
            "conjunction\t9595\t0.00053\n",
            "with\t2007\t0.00205\n",
            "a\t1037\t0.51893\n",
            "1915\t4936\t0.00159\n",
            "showing\t4760\t0.00020\n",
            "of\t1997\t0.03841\n",
            "the\t1996\t0.12740\n",
            "film\t2143\t0.00008\n",
            "at\t2012\t0.14402\n",
            "carnegie\t11298\t0.00021\n",
            "hall\t2534\t0.00036\n",
            ":\t1024\t0.00004\n",
            "pu\t16405\t0.04768\n",
            "##g\t2290\t0.00012\n",
            ",\t1010\t0.99678\n",
            "a\t1037\t0.90272\n",
            "down\t2091\t0.77989\n",
            "-\t1011\t0.99874\n",
            "and\t1998\t0.00004\n",
            "-\t1011\t0.00137\n",
            "out\t2041\t0.00009\n",
            "ho\t7570\t0.01612\n",
            "##bo\t5092\t0.00002\n",
            ",\t1010\t0.07719\n",
            "is\t2003\t0.01842\n",
            "talked\t5720\t0.00276\n",
            "into\t2046\t0.00377\n",
            "pretending\t12097\t0.00020\n",
            "he\t2002\t0.35495\n",
            "is\t2003\t0.04952\n",
            "cyclone\t11609\t0.00001\n",
            "flynn\t13259\t0.00011\n",
            ",\t1010\t0.39452\n",
            "the\t1996\t0.00186\n",
            "boxing\t8362\t0.02496\n",
            "champion\t3410\t0.00006\n",
            ",\t1010\t0.03915\n",
            "and\t1998\t0.03877\n",
            "entering\t5738\t0.00051\n",
            "the\t1996\t0.96106\n",
            "ring\t3614\t0.00002\n",
            "for\t2005\t0.01275\n",
            "a\t1037\t0.03844\n",
            "fight\t2954\t0.00002\n",
            ".\t1012\t0.67746\n",
            "we\t2057\t0.19515\n",
            "are\t2024\t0.00905\n",
            "told\t2409\t0.00000\n",
            "charlie\t4918\t0.00001\n",
            "is\t2003\t0.39483\n",
            "a\t1037\t0.06096\n",
            "dental\t11394\t0.00193\n",
            "assistant\t3353\t0.00002\n",
            ".\t1012\t0.96486\n",
            "charlie\t4918\t0.00547\n",
            "offers\t4107\t0.96337\n",
            "mabel\t19486\t0.00000\n",
            "a\t1037\t0.19521\n",
            "ride\t4536\t0.00363\n",
            "on\t2006\t0.20685\n",
            "his\t2010\t0.07732\n",
            "two\t2048\t0.49186\n",
            "-\t1011\t0.00027\n",
            "seater\t23392\t0.00003\n",
            "motorcycle\t9055\t0.00119\n",
            ",\t1010\t0.95218\n",
            "which\t2029\t0.16303\n",
            "she\t2016\t0.00626\n",
            "accepts\t13385\t0.00004\n",
            "in\t1999\t0.05622\n",
            "preference\t12157\t0.00002\n",
            "to\t2000\t0.04668\n",
            "his\t2010\t0.27881\n",
            "rival\t6538\t0.00003\n",
            "'\t1005\t0.99830\n",
            "s\t1055\t0.00070\n",
            "racing\t3868\t0.00003\n",
            "car\t2482\t0.00008\n",
            ".\t1012\t0.82102\n",
            "mabel\t19486\t0.00103\n",
            "'\t1005\t0.99957\n",
            "s\t1055\t0.00032\n",
            "blu\t14154\t0.15057\n",
            "##nder\t11563\t0.00001\n",
            "tells\t4136\t0.23548\n",
            "the\t1996\t0.89402\n",
            "tale\t6925\t0.01841\n",
            "of\t1997\t0.00025\n",
            "a\t1037\t0.72324\n",
            "young\t2402\t0.43141\n",
            "woman\t2450\t0.04408\n",
            "who\t2040\t0.38209\n",
            "is\t2003\t0.33845\n",
            "secretly\t10082\t0.01126\n",
            "engaged\t5117\t0.03397\n",
            "to\t2000\t0.00039\n",
            "the\t1996\t0.81498\n",
            "boss\t5795\t0.00008\n",
            "'\t1005\t0.99902\n",
            "s\t1055\t0.00047\n",
            "son\t2365\t0.00007\n",
            ".\t1012\t0.07651\n",
            "[\t1031\t0.98031\n",
            "1\t1015\t0.00325\n",
            "]\t1033\t0.00001\n",
            "chaplin\t23331\t0.00031\n",
            ",\t1010\t0.90123\n",
            "in\t1999\t0.97329\n",
            "tram\t12517\t0.21023\n",
            "##p\t2361\t0.00000\n",
            "attire\t20426\t0.00001\n",
            ",\t1010\t0.04820\n",
            "sits\t7719\t0.00310\n",
            "in\t1999\t0.60643\n",
            "the\t1996\t0.26259\n",
            "park\t2380\t0.00003\n",
            "with\t2007\t0.12289\n",
            "his\t2010\t0.89856\n",
            "wife\t2564\t0.06723\n",
            ",\t1010\t0.10724\n",
            "mabel\t19486\t0.00001\n",
            ".\t1012\t0.89452\n",
            "in\t1999\t0.94781\n",
            "a\t1037\t0.16840\n",
            "hotel\t3309\t0.00152\n",
            "lobby\t9568\t0.00000\n",
            "a\t1037\t0.13282\n",
            "heavily\t4600\t0.00087\n",
            "drunk\t7144\t0.00017\n",
            "tram\t12517\t0.01961\n",
            "##p\t2361\t0.00005\n",
            "runs\t3216\t0.04469\n",
            "into\t2046\t0.00816\n",
            "an\t2019\t0.29130\n",
            "elegant\t11552\t0.00016\n",
            "lady\t3203\t0.00076\n",
            ",\t1010\t0.01721\n",
            "mabel\t19486\t0.00010\n",
            ",\t1010\t0.38307\n",
            "who\t2040\t0.00439\n",
            "gets\t4152\t0.00530\n",
            "tied\t5079\t0.03988\n",
            "up\t2039\t0.08467\n",
            "in\t1999\t0.01492\n",
            "her\t2014\t0.01723\n",
            "dog\t3899\t0.00505\n",
            "'\t1005\t0.99948\n",
            "s\t1055\t0.00004\n",
            "leash\t26834\t0.00000\n",
            ",\t1010\t0.10297\n",
            "and\t1998\t0.00725\n",
            "falls\t4212\t0.00015\n",
            "down\t2091\t0.00023\n",
            ".\t1012\t0.68000\n",
            "chaplin\t23331\t0.00241\n",
            "'\t1005\t0.99995\n",
            "s\t1055\t0.00031\n",
            "character\t2839\t0.00038\n",
            "attempts\t4740\t0.15816\n",
            "to\t2000\t0.87457\n",
            "convince\t8054\t0.00372\n",
            "a\t1037\t0.00229\n",
            "pass\t3413\t0.01787\n",
            "##er\t2121\t0.00019\n",
            "##by\t3762\t0.00229\n",
            "(\t1006\t0.11742\n",
            "director\t2472\t0.00102\n",
            "henry\t2888\t0.00355\n",
            "le\t3393\t0.15373\n",
            "##hr\t8093\t0.00000\n",
            "##man\t2386\t0.00113\n",
            ")\t1007\t0.00075\n",
            "to\t2000\t0.57355\n",
            "give\t2507\t0.04462\n",
            "him\t2032\t0.00117\n",
            "money\t2769\t0.00004\n",
            ".\t1012\t0.85749\n",
            "the\t1996\t0.93690\n",
            "mas\t16137\t0.99966\n",
            "##que\t4226\t0.00037\n",
            "##rade\t13662\t0.00025\n",
            "##r\t2099\t0.00043\n",
            "is\t2003\t0.94206\n",
            "a\t1037\t0.00883\n",
            "comedy\t4038\t0.01273\n",
            "short\t2460\t0.00000\n",
            "whose\t3005\t0.00075\n",
            "plot\t5436\t0.00072\n",
            "revolves\t19223\t0.00245\n",
            "around\t2105\t0.00007\n",
            "making\t2437\t0.00058\n",
            "films\t3152\t0.00002\n",
            "at\t2012\t0.03469\n",
            "keystone\t22271\t0.00002\n",
            ".\t1012\t0.88054\n",
            "the\t1996\t0.89701\n",
            "daughter\t2684\t0.50394\n",
            "of\t1997\t0.00011\n",
            "king\t2332\t0.00274\n",
            "neptune\t21167\t0.00000\n",
            "takes\t3138\t0.83496\n",
            "on\t2006\t0.00065\n",
            "human\t2529\t0.50380\n",
            "form\t2433\t0.00002\n",
            "to\t2000\t0.93067\n",
            "avenge\t24896\t0.00201\n",
            "the\t1996\t0.99636\n",
            "death\t2331\t0.00096\n",
            "of\t1997\t0.01503\n",
            "her\t2014\t0.29171\n",
            "young\t2402\t0.00416\n",
            "sister\t2905\t0.00002\n",
            ",\t1010\t0.53005\n",
            "who\t2040\t0.00397\n",
            "was\t2001\t0.27296\n",
            "caught\t3236\t0.31129\n",
            "in\t1999\t0.10094\n",
            "a\t1037\t0.06426\n",
            "fishing\t5645\t0.00000\n",
            "net\t5658\t0.00007\n",
            ".\t1012\t0.80116\n",
            "the\t1996\t0.90439\n",
            "hero\t5394\t0.00003\n",
            ",\t1010\t0.98528\n",
            "a\t1037\t0.00689\n",
            "jan\t5553\t0.99371\n",
            "##itor\t15660\t0.00001\n",
            "played\t2209\t0.03123\n",
            "by\t2011\t0.00019\n",
            "chaplin\t23331\t0.00003\n",
            ",\t1010\t0.08835\n",
            "is\t2003\t0.56959\n",
            "fired\t5045\t0.25821\n",
            "from\t2013\t0.03959\n",
            "work\t2147\t0.00165\n",
            "for\t2005\t0.14004\n",
            "accidentally\t9554\t0.00513\n",
            "knocking\t10591\t0.00118\n",
            "his\t2010\t0.02000\n",
            "bucket\t13610\t0.08578\n",
            "of\t1997\t0.00289\n",
            "water\t2300\t0.00004\n",
            "out\t2041\t0.77020\n",
            "the\t1996\t0.64885\n",
            "window\t3332\t0.00002\n",
            "and\t1998\t0.00041\n",
            "onto\t3031\t0.00001\n",
            "his\t2010\t0.06173\n",
            "boss\t5795\t0.00022\n",
            ",\t1010\t0.06498\n",
            "the\t1996\t0.04265\n",
            "chief\t2708\t0.00111\n",
            "banker\t13448\t0.00001\n",
            "(\t1006\t0.02139\n",
            "tan\t9092\t0.11498\n",
            "##dy\t5149\t0.00003\n",
            ")\t1007\t0.02851\n",
            ".\t1012\t0.67045\n",
            "the\t1996\t0.99291\n",
            "premise\t18458\t0.06201\n",
            "of\t1997\t0.52891\n",
            "the\t1996\t0.03681\n",
            "story\t2466\t0.35192\n",
            "was\t2001\t0.00001\n",
            "that\t2008\t0.00133\n",
            "pauline\t15595\t0.00109\n",
            "'\t1005\t0.99981\n",
            "s\t1055\t0.00010\n",
            "wealthy\t7272\t0.00350\n",
            "guardian\t6697\t0.00001\n",
            "mr\t2720\t0.00112\n",
            ".\t1012\t0.70516\n",
            "marvin\t13748\t0.00000\n",
            ",\t1010\t0.92074\n",
            "upon\t2588\t0.07182\n",
            "his\t2010\t0.54233\n",
            "death\t2331\t0.00001\n",
            ",\t1010\t0.05199\n",
            "has\t2038\t0.02715\n",
            "left\t2187\t0.43216\n",
            "her\t2014\t0.73424\n",
            "inheritance\t12839\t0.00000\n",
            "in\t1999\t0.26193\n",
            "the\t1996\t0.41718\n",
            "care\t2729\t0.00055\n",
            "of\t1997\t0.03278\n",
            "his\t2010\t0.42877\n",
            "secretary\t3187\t0.00175\n",
            ",\t1010\t0.08953\n",
            "mr\t2720\t0.00568\n",
            ".\t1012\t0.02003\n",
            "ko\t12849\t0.30199\n",
            "##ern\t11795\t0.00004\n",
            "##er\t2121\t0.00170\n",
            ",\t1010\t0.01558\n",
            "until\t2127\t0.00887\n",
            "the\t1996\t0.98127\n",
            "time\t2051\t0.00374\n",
            "of\t1997\t0.02183\n",
            "her\t2014\t0.03813\n",
            "marriage\t3510\t0.00004\n",
            ".\t1012\t0.58025\n",
            "charlie\t4918\t0.00065\n",
            "is\t2003\t0.94268\n",
            "in\t1999\t0.63358\n",
            "charge\t3715\t0.00103\n",
            "of\t1997\t0.00239\n",
            "stage\t2754\t0.00115\n",
            "\"\t1000\t0.00633\n",
            "props\t24387\t0.00003\n",
            "\"\t1000\t0.00223\n",
            "and\t1998\t0.00445\n",
            "has\t2038\t0.23283\n",
            "trouble\t4390\t0.00096\n",
            "with\t2007\t0.00118\n",
            "actors\t5889\t0.03474\n",
            "'\t1005\t0.00086\n",
            "luggage\t17434\t0.00001\n",
            "and\t1998\t0.00949\n",
            "conflicts\t9755\t0.00048\n",
            "over\t2058\t0.00013\n",
            "who\t2040\t0.00464\n",
            "gets\t4152\t0.00002\n",
            "the\t1996\t0.38778\n",
            "star\t2732\t0.00048\n",
            "'\t1005\t0.99544\n",
            "s\t1055\t0.00160\n",
            "dressing\t11225\t0.01283\n",
            "room\t2282\t0.00019\n",
            ".\t1012\t0.75336\n",
            "seated\t8901\t0.00329\n",
            "in\t1999\t0.74371\n",
            "a\t1037\t0.02216\n",
            "park\t2380\t0.00001\n",
            ",\t1010\t0.89352\n",
            "charlie\t4918\t0.00016\n",
            "gives\t3957\t0.00009\n",
            "his\t2010\t0.44197\n",
            "expert\t6739\t0.00000\n",
            "attention\t3086\t0.00153\n",
            "to\t2000\t0.01124\n",
            "the\t1996\t0.24035\n",
            "picture\t3861\t0.24490\n",
            "of\t1997\t0.00936\n",
            "a\t1037\t0.51338\n",
            "pretty\t3492\t0.00303\n",
            "girl\t2611\t0.00007\n",
            "on\t2006\t0.20115\n",
            "the\t1996\t0.92387\n",
            "cover\t3104\t0.00009\n",
            "of\t1997\t0.01855\n",
            "the\t1996\t0.38842\n",
            "police\t2610\t0.00008\n",
            "gazette\t11391\t0.00001\n",
            ".\t1012\t0.82102\n",
            "es\t9686\t0.00030\n",
            "##ra\t2527\t0.00019\n",
            "kincaid\t24510\t0.00006\n",
            "(\t1006\t0.40447\n",
            "la\t2474\t0.01166\n",
            "reno\t17738\t0.00001\n",
            ")\t1007\t0.00786\n",
            "takes\t3138\t0.00000\n",
            "land\t2455\t0.00029\n",
            "by\t2011\t0.18559\n",
            "force\t2486\t0.00002\n",
            "and\t1998\t0.21894\n",
            ",\t1010\t0.17559\n",
            "having\t2383\t0.04046\n",
            "taken\t2579\t0.00000\n",
            "the\t1996\t0.38181\n",
            "es\t9686\t0.99925\n",
            "##pin\t8091\t0.00031\n",
            "##oza\t25036\t0.00000\n",
            "land\t2455\t0.00022\n",
            ",\t1010\t0.17692\n",
            "his\t2010\t0.20611\n",
            "sights\t15925\t0.00001\n",
            "are\t2024\t0.20388\n",
            "set\t2275\t0.00090\n",
            "on\t2006\t0.20776\n",
            "the\t1996\t0.02516\n",
            "castro\t11794\t0.00003\n",
            "rancho\t18123\t0.00000\n",
            ".\t1012\t0.67953\n",
            "a\t1037\t0.87667\n",
            "drunk\t7144\t0.02413\n",
            "rev\t7065\t0.22737\n",
            "##eller\t24038\t0.00000\n",
            "(\t1006\t0.00015\n",
            "chaplin\t23331\t0.00092\n",
            ")\t1007\t0.00033\n",
            "returns\t5651\t0.52268\n",
            "home\t2188\t0.00047\n",
            "to\t2000\t0.02501\n",
            "a\t1037\t0.17774\n",
            "sc\t8040\t0.99916\n",
            "##old\t11614\t0.00003\n",
            "##ing\t2075\t0.00074\n",
            "from\t2013\t0.00297\n",
            "his\t2010\t0.51550\n",
            "wife\t2564\t0.00002\n",
            ".\t1012\t0.88054\n",
            "rough\t5931\t0.99163\n",
            "-\t1011\t0.68038\n",
            "and\t1998\t0.00145\n",
            "-\t1011\t0.02329\n",
            "tumble\t28388\t0.00000\n",
            "gold\t2751\t0.94626\n",
            "rush\t5481\t0.00014\n",
            "-\t1011\t0.28341\n",
            "era\t3690\t0.00011\n",
            "california\t2662\t0.00007\n",
            ":\t1024\t0.14884\n",
            "a\t1037\t0.00112\n",
            "woman\t2450\t0.00014\n",
            "(\t1006\t0.75197\n",
            "sal\t16183\t0.03274\n",
            "##omy\t16940\t0.00000\n",
            "jane\t4869\t0.00025\n",
            ")\t1007\t0.08073\n",
            "is\t2003\t0.47366\n",
            "saved\t5552\t0.67879\n",
            "from\t2013\t0.01705\n",
            "a\t1037\t0.37566\n",
            "ru\t21766\t0.99694\n",
            "##ffi\t26989\t0.00002\n",
            "##an\t2319\t0.00152\n",
            "(\t1006\t0.07858\n",
            "red\t2417\t0.00217\n",
            "pete\t6969\t0.00002\n",
            ")\t1007\t0.00002\n",
            "by\t2011\t0.01974\n",
            "a\t1037\t0.08191\n",
            "heroic\t14779\t0.00002\n",
            "stranger\t7985\t0.00003\n",
            "(\t1006\t0.04297\n",
            "jack\t2990\t0.00595\n",
            "dart\t14957\t0.00001\n",
            ")\t1007\t0.00992\n",
            ",\t1010\t0.23214\n",
            "the\t1996\t0.98178\n",
            "latter\t3732\t0.00001\n",
            "saved\t5552\t0.07426\n",
            "from\t2013\t0.02377\n",
            "a\t1037\t0.01485\n",
            "lynch\t11404\t0.00760\n",
            "##ing\t2075\t0.00007\n",
            "when\t2043\t0.00266\n",
            "falsely\t23123\t0.00964\n",
            "accused\t5496\t0.00338\n",
            "of\t1997\t0.06377\n",
            "a\t1037\t0.00073\n",
            "crime\t4126\t0.00004\n",
            ".\t1012\t0.58224\n",
            "my\t2026\t0.95929\n",
            "wife\t2564\t0.00050\n",
            "took\t2165\t0.04635\n",
            "me\t2033\t0.00767\n",
            "here\t2182\t0.04929\n",
            "on\t2006\t0.19633\n",
            "my\t2026\t0.26476\n",
            "birthday\t5798\t0.00009\n",
            "for\t2005\t0.52925\n",
            "breakfast\t6350\t0.00012\n",
            "and\t1998\t0.16648\n",
            "it\t2009\t0.10616\n",
            "was\t2001\t0.02273\n",
            "excellent\t6581\t0.00000\n",
            ".\t1012\t0.91250\n",
            "i\t1045\t0.97149\n",
            "have\t2031\t0.71527\n",
            "no\t2053\t0.95569\n",
            "idea\t2801\t0.00002\n",
            "why\t2339\t0.00052\n",
            "some\t2070\t0.07396\n",
            "people\t2111\t0.00289\n",
            "give\t2507\t0.00000\n",
            "bad\t2919\t0.00033\n",
            "reviews\t4391\t0.00000\n",
            "about\t2055\t0.02586\n",
            "this\t2023\t0.08898\n",
            "place\t2173\t0.00000\n",
            ".\t1012\t0.91564\n",
            "love\t2293\t0.00022\n",
            "the\t1996\t0.00740\n",
            "g\t1043\t0.76349\n",
            "##yr\t12541\t0.00000\n",
            "##o\t2080\t0.00002\n",
            "plate\t5127\t0.00001\n",
            ".\t1012\t0.98182\n",
            "rosie\t15820\t0.00006\n",
            ",\t1010\t0.29006\n",
            "dakota\t7734\t0.00002\n",
            ",\t1010\t0.14770\n",
            "and\t1998\t0.00651\n",
            "i\t1045\t0.10494\n",
            "love\t2293\t0.00005\n",
            "cha\t15775\t0.99971\n",
            "##par\t19362\t0.00008\n",
            "##ral\t7941\t0.00000\n",
            "dog\t3899\t0.00141\n",
            "park\t2380\t0.00001\n",
            "!\t999\t0.84098\n",
            "!\t999\t0.14004\n",
            "!\t999\t0.01134\n",
            "general\t2236\t0.07695\n",
            "manager\t3208\t0.00002\n",
            "scott\t3660\t0.00001\n",
            "pete\t6969\t0.00007\n",
            "##llo\t7174\t0.00000\n",
            "is\t2003\t0.03274\n",
            "a\t1037\t0.24301\n",
            "good\t2204\t0.00068\n",
            "egg\t8288\t0.00000\n",
            "!\t999\t0.84709\n",
            "!\t999\t0.11351\n",
            "!\t999\t0.01351\n",
            "qui\t21864\t0.00000\n",
            "##ess\t7971\t0.00002\n",
            "##ence\t10127\t0.00000\n",
            "is\t2003\t0.00488\n",
            ",\t1010\t0.10582\n",
            "simply\t3432\t0.02480\n",
            "put\t2404\t0.00001\n",
            ",\t1010\t0.00804\n",
            "beautiful\t3376\t0.00001\n",
            ".\t1012\t0.96043\n",
            "drop\t4530\t0.00464\n",
            "what\t2054\t0.55569\n",
            "you\t2017\t0.84103\n",
            "'\t1005\t0.99959\n",
            "re\t2128\t0.00002\n",
            "doing\t2725\t0.00001\n",
            "and\t1998\t0.04541\n",
            "drive\t3298\t0.00001\n",
            "here\t2182\t0.00001\n",
            ".\t1012\t0.96043\n",
            "luckily\t15798\t0.31982\n",
            ",\t1010\t0.89301\n",
            "i\t1045\t0.98911\n",
            "didn\t2134\t0.70098\n",
            "'\t1005\t0.88075\n",
            "t\t1056\t0.00003\n",
            "have\t2031\t0.02917\n",
            "to\t2000\t0.82490\n",
            "travel\t3604\t0.00000\n",
            "far\t2521\t0.00002\n",
            "to\t2000\t0.80844\n",
            "make\t2191\t0.00068\n",
            "my\t2026\t0.00051\n",
            "connecting\t7176\t0.00006\n",
            "flight\t3462\t0.00002\n",
            ".\t1012\t0.90057\n",
            "definitely\t5791\t0.00015\n",
            "come\t2272\t0.00040\n",
            "for\t2005\t0.00790\n",
            "happy\t3407\t0.00401\n",
            "hour\t3178\t0.00001\n",
            "!\t999\t0.00569\n",
            "no\t2053\t0.92917\n",
            "##bu\t8569\t0.00008\n",
            "##o\t2080\t0.00742\n",
            "shows\t3065\t0.00043\n",
            "his\t2010\t0.15297\n",
            "unique\t4310\t0.00032\n",
            "talents\t11725\t0.00000\n",
            "with\t2007\t0.01465\n",
            "everything\t2673\t0.00155\n",
            "on\t2006\t0.02301\n",
            "the\t1996\t0.41765\n",
            "menu\t12183\t0.00000\n",
            ".\t1012\t0.92896\n",
            "the\t1996\t0.98193\n",
            "old\t2214\t0.00384\n",
            "##ish\t4509\t0.00034\n",
            "man\t2158\t0.05032\n",
            "who\t2040\t0.73648\n",
            "owns\t8617\t0.00004\n",
            "the\t1996\t0.10450\n",
            "store\t3573\t0.00008\n",
            "is\t2003\t0.03365\n",
            "as\t2004\t0.94095\n",
            "sweet\t4086\t0.00501\n",
            "as\t2004\t0.00020\n",
            "can\t2064\t0.05890\n",
            "be\t2022\t0.00002\n",
            ".\t1012\t0.91250\n",
            "wonderful\t6919\t0.00004\n",
            "vietnamese\t9101\t0.00005\n",
            "sandwich\t11642\t0.00229\n",
            "shop\t4497\t0.01494\n",
            "##pe\t5051\t0.00000\n",
            ".\t1012\t0.98295\n",
            "they\t2027\t0.01218\n",
            "have\t2031\t0.65237\n",
            "a\t1037\t0.54590\n",
            "limited\t3132\t0.00472\n",
            "time\t2051\t0.00017\n",
            "thing\t2518\t0.00174\n",
            "going\t2183\t0.28962\n",
            "on\t2006\t0.01683\n",
            "right\t2157\t0.20542\n",
            "now\t2085\t0.00351\n",
            "with\t2007\t0.01088\n",
            "bb\t22861\t0.99773\n",
            "##q\t4160\t0.00000\n",
            "chicken\t7975\t0.00087\n",
            "pizza\t10733\t0.00007\n",
            "(\t1006\t0.85708\n",
            "not\t2025\t0.99794\n",
            "sure\t2469\t0.00056\n",
            "how\t2129\t0.95784\n",
            "long\t2146\t0.00004\n",
            "it\t2009\t0.87952\n",
            "'\t1005\t0.99907\n",
            "s\t1055\t0.00154\n",
            "going\t2183\t0.02990\n",
            "to\t2000\t0.00228\n",
            "last\t2197\t0.00012\n",
            ")\t1007\t0.00957\n",
            "but\t2021\t0.03756\n",
            "let\t2292\t0.38838\n",
            "me\t2033\t0.00062\n",
            "just\t2074\t0.02095\n",
            "say\t2360\t0.00129\n",
            "it\t2009\t0.14915\n",
            "was\t2001\t0.03037\n",
            "amazing\t6429\t0.00001\n",
            ".\t1012\t0.67087\n",
            "good\t2204\t0.00010\n",
            "tattoo\t11660\t0.00587\n",
            "shop\t4497\t0.00003\n",
            ".\t1012\t0.85425\n",
            "i\t1045\t0.99982\n",
            "'\t1005\t0.95590\n",
            "m\t1049\t0.00004\n",
            "2\t1016\t0.00150\n",
            "weeks\t3134\t0.00005\n",
            "new\t2047\t0.00025\n",
            "to\t2000\t0.00390\n",
            "phoenix\t6708\t0.00000\n",
            ".\t1012\t0.96486\n",
            "was\t2001\t0.88341\n",
            "it\t2009\t0.14261\n",
            "worth\t4276\t0.00139\n",
            "the\t1996\t0.00185\n",
            "21\t2538\t0.00013\n",
            "$\t1002\t0.00000\n",
            "for\t2005\t0.33793\n",
            "a\t1037\t0.00681\n",
            "salad\t16521\t0.01862\n",
            "and\t1998\t0.00181\n",
            "small\t2235\t0.00024\n",
            "pizza\t10733\t0.00006\n",
            "?\t1029\t0.01412\n",
            "we\t2057\t0.19698\n",
            "went\t2253\t0.00596\n",
            "here\t2182\t0.00146\n",
            "on\t2006\t0.88672\n",
            "a\t1037\t0.00238\n",
            "saturday\t5095\t0.02365\n",
            "afternoon\t5027\t0.00000\n",
            "and\t1998\t0.02650\n",
            "this\t2023\t0.09878\n",
            "place\t2173\t0.09703\n",
            "was\t2001\t0.06204\n",
            "incredibly\t11757\t0.00003\n",
            "empty\t4064\t0.00000\n",
            ".\t1012\t0.91564\n",
            "okay\t3100\t0.00005\n",
            "this\t2023\t0.55896\n",
            "is\t2003\t0.64005\n",
            "the\t1996\t0.94410\n",
            "best\t2190\t0.53953\n",
            "place\t2173\t0.00001\n",
            "ever\t2412\t0.00006\n",
            "!\t999\t0.00900\n",
            "i\t1045\t0.91790\n",
            "met\t2777\t0.27488\n",
            "a\t1037\t0.69393\n",
            "friend\t2767\t0.00010\n",
            "for\t2005\t0.77383\n",
            "lunch\t6265\t0.00082\n",
            "yesterday\t7483\t0.00000\n",
            ".\t1012\t0.97217\n",
            "they\t2027\t0.12069\n",
            "'\t1005\t0.99988\n",
            "ve\t2310\t0.00002\n",
            "gotten\t5407\t0.28733\n",
            "better\t2488\t0.86717\n",
            "and\t1998\t0.00022\n",
            "better\t2488\t0.00359\n",
            "for\t2005\t0.02589\n",
            "me\t2033\t0.00007\n",
            "in\t1999\t0.45704\n",
            "the\t1996\t0.00336\n",
            "time\t2051\t0.00002\n",
            "since\t2144\t0.00448\n",
            "this\t2023\t0.00209\n",
            "review\t3319\t0.00161\n",
            "was\t2001\t0.02885\n",
            "written\t2517\t0.00001\n",
            ".\t1012\t0.88054\n",
            "d\t1040\t0.04156\n",
            "##va\t3567\t0.00016\n",
            "##p\t2361\t0.00006\n",
            ".\t1012\t0.30024\n",
            ".\t1012\t0.91924\n",
            ".\t1012\t0.00024\n",
            ".\t1012\t0.57254\n",
            "you\t2017\t0.81632\n",
            "have\t2031\t0.60349\n",
            "to\t2000\t0.03420\n",
            "go\t2175\t0.00195\n",
            "at\t2012\t0.99971\n",
            "least\t2560\t0.00007\n",
            "once\t2320\t0.00107\n",
            "in\t1999\t0.11380\n",
            "your\t2115\t0.00154\n",
            "life\t2166\t0.00004\n",
            ".\t1012\t0.88054\n",
            "this\t2023\t0.68968\n",
            "place\t2173\t0.00019\n",
            "shouldn\t5807\t0.21624\n",
            "'\t1005\t0.97016\n",
            "t\t1056\t0.00004\n",
            "even\t2130\t0.00017\n",
            "be\t2022\t0.00223\n",
            "reviewed\t8182\t0.00000\n",
            "-\t1011\t0.02571\n",
            "because\t2138\t0.02999\n",
            "it\t2009\t0.04815\n",
            "is\t2003\t0.27037\n",
            "the\t1996\t0.81098\n",
            "kind\t2785\t0.76152\n",
            "of\t1997\t0.00031\n",
            "place\t2173\t0.00004\n",
            "i\t1045\t0.96462\n",
            "want\t2215\t0.02412\n",
            "to\t2000\t0.12421\n",
            "keep\t2562\t0.00032\n",
            "for\t2005\t0.03046\n",
            "myself\t2870\t0.00002\n",
            ".\t1012\t0.77131\n",
            ".\t1012\t0.05725\n",
            ".\t1012\t0.01390\n",
            "=\t1027\t0.00017\n",
            ")\t1007\t0.00019\n",
            "first\t2034\t0.01159\n",
            "time\t2051\t0.00411\n",
            "my\t2026\t0.33174\n",
            "friend\t2767\t0.00010\n",
            "and\t1998\t0.07880\n",
            "i\t1045\t0.29881\n",
            "went\t2253\t0.00181\n",
            "there\t2045\t0.00002\n",
            ".\t1012\t0.96077\n",
            ".\t1012\t0.00009\n",
            ".\t1012\t0.94415\n",
            "u\t1057\t0.00005\n",
            "can\t2064\t0.00433\n",
            "go\t2175\t0.11419\n",
            "there\t2045\t0.00126\n",
            "n\t1050\t0.00000\n",
            "check\t4638\t0.00042\n",
            "the\t1996\t0.77725\n",
            "car\t2482\t0.00003\n",
            "out\t2041\t0.00000\n",
            ".\t1012\t0.96043\n",
            "i\t1045\t0.98782\n",
            "love\t2293\t0.02968\n",
            "this\t2023\t0.13571\n",
            "place\t2173\t0.00022\n",
            "!\t999\t0.01947\n",
            "“\t1523\t0.00000\n",
            "i\t1045\t0.85133\n",
            "had\t2018\t0.06887\n",
            "then\t2059\t0.00242\n",
            ",\t1010\t0.35069\n",
            "as\t2004\t0.03527\n",
            "you\t2017\t0.08765\n",
            "remember\t3342\t0.00045\n",
            ",\t1010\t0.00573\n",
            "just\t2074\t0.00081\n",
            "returned\t2513\t0.02119\n",
            "to\t2000\t0.10477\n",
            "london\t2414\t0.00000\n",
            "after\t2044\t0.00071\n",
            "a\t1037\t0.99067\n",
            "lot\t2843\t0.00076\n",
            "of\t1997\t0.00106\n",
            "indian\t2796\t0.24333\n",
            "ocean\t4153\t0.00367\n",
            ",\t1010\t0.17975\n",
            "pacific\t3534\t0.00131\n",
            ",\t1010\t0.01140\n",
            "china\t2859\t0.00017\n",
            "seas\t11915\t0.00000\n",
            "-\t1011\t0.99446\n",
            "-\t1011\t0.00328\n",
            "a\t1037\t0.90283\n",
            "regular\t3180\t0.00128\n",
            "dose\t13004\t0.00000\n",
            "of\t1997\t0.00712\n",
            "the\t1996\t0.16562\n",
            "east\t2264\t0.00005\n",
            "-\t1011\t0.00000\n",
            "-\t1011\t0.00501\n",
            "six\t2416\t0.03951\n",
            "years\t2086\t0.01687\n",
            "or\t2030\t0.00498\n",
            "so\t2061\t0.00096\n",
            ",\t1010\t0.67883\n",
            "and\t1998\t0.12875\n",
            "i\t1045\t0.94074\n",
            "was\t2001\t0.00938\n",
            "loaf\t27048\t0.00397\n",
            "##ing\t2075\t0.00006\n",
            "about\t2055\t0.00010\n",
            ",\t1010\t0.05680\n",
            "hind\t17666\t0.29264\n",
            "##ering\t7999\t0.00002\n",
            "you\t2017\t0.03370\n",
            "fellows\t13572\t0.00008\n",
            "in\t1999\t0.13010\n",
            "your\t2115\t0.95041\n",
            "work\t2147\t0.00007\n",
            "and\t1998\t0.00262\n",
            "invading\t17657\t0.00000\n",
            "your\t2115\t0.25383\n",
            "homes\t5014\t0.00000\n",
            ",\t1010\t0.43448\n",
            "just\t2074\t0.00254\n",
            "as\t2004\t0.08285\n",
            "though\t2295\t0.00081\n",
            "i\t1045\t0.08252\n",
            "had\t2018\t0.00031\n",
            "got\t2288\t0.00095\n",
            "a\t1037\t0.09207\n",
            "heavenly\t16581\t0.00025\n",
            "mission\t3260\t0.00046\n",
            "to\t2000\t0.32355\n",
            "civil\t2942\t0.01151\n",
            "##ize\t4697\t0.00000\n",
            "you\t2017\t0.00014\n",
            ".\t1012\t0.53724\n",
            "he\t2002\t0.89793\n",
            "had\t2018\t0.77616\n",
            ",\t1010\t0.99841\n",
            "of\t1997\t0.99970\n",
            "course\t2607\t0.00000\n",
            ",\t1010\t0.00030\n",
            "dreamed\t13830\t0.00609\n",
            "of\t1997\t0.23814\n",
            "battles\t7465\t0.00023\n",
            "all\t2035\t0.00008\n",
            "his\t2010\t0.95570\n",
            "life\t2166\t0.00081\n",
            "-\t1011\t0.00408\n",
            "-\t1011\t0.00005\n",
            "of\t1997\t0.00292\n",
            "vague\t13727\t0.00028\n",
            "and\t1998\t0.00019\n",
            "bloody\t6703\t0.00008\n",
            "conflicts\t9755\t0.00002\n",
            "that\t2008\t0.00267\n",
            "had\t2018\t0.27670\n",
            "thrilled\t16082\t0.00004\n",
            "him\t2032\t0.00540\n",
            "with\t2007\t0.08365\n",
            "their\t2037\t0.00138\n",
            "sweep\t11740\t0.00001\n",
            "and\t1998\t0.00933\n",
            "fire\t2543\t0.00013\n",
            ".\t1012\t0.71784\n",
            "these\t2122\t0.06124\n",
            "domestic\t4968\t0.00110\n",
            "pilgrimage\t14741\t0.00000\n",
            "##s\t2015\t0.11477\n",
            "were\t2020\t0.07146\n",
            "invariably\t26597\t0.00000\n",
            "in\t1999\t0.00573\n",
            "state\t2110\t0.00000\n",
            ";\t1025\t0.00608\n",
            "two\t2048\t0.00235\n",
            "maids\t29229\t0.00001\n",
            ",\t1010\t0.00587\n",
            "the\t1996\t0.07952\n",
            "private\t2797\t0.00307\n",
            "car\t2482\t0.00001\n",
            ",\t1010\t0.64320\n",
            "or\t2030\t0.00825\n",
            "mr\t2720\t0.06368\n",
            ".\t1012\t0.00295\n",
            "blaine\t20002\t0.00000\n",
            "when\t2043\t0.00108\n",
            "available\t2800\t0.00000\n",
            ",\t1010\t0.53126\n",
            "and\t1998\t0.12528\n",
            "very\t2200\t0.03973\n",
            "often\t2411\t0.00032\n",
            "a\t1037\t0.12299\n",
            "physician\t7522\t0.00001\n",
            ".\t1012\t0.71784\n",
            "the\t1996\t0.76217\n",
            "monthly\t7058\t0.00000\n",
            "nurse\t6821\t0.00682\n",
            "tried\t2699\t0.15345\n",
            "to\t2000\t0.98989\n",
            "quiet\t4251\t0.00002\n",
            "her\t2014\t0.00119\n",
            ",\t1010\t0.76403\n",
            "and\t1998\t0.38071\n",
            "presently\t12825\t0.00002\n",
            ",\t1010\t0.09577\n",
            "from\t2013\t0.12355\n",
            "exhaustion\t15575\t0.00014\n",
            ",\t1010\t0.17858\n",
            "the\t1996\t0.86142\n",
            "crying\t6933\t0.00003\n",
            "ceased\t7024\t0.00000\n",
            ".\t1012\t0.88054\n",
            "and\t1998\t0.17562\n",
            "before\t2077\t0.67676\n",
            "we\t2057\t0.67533\n",
            "judge\t3648\t0.00003\n",
            "of\t1997\t0.00139\n",
            "them\t2068\t0.00071\n",
            "too\t2205\t0.04823\n",
            "harshly\t21052\t0.00000\n",
            "we\t2057\t0.67729\n",
            "must\t2442\t0.02518\n",
            "remember\t3342\t0.00005\n",
            "what\t2054\t0.00053\n",
            "ruthless\t18101\t0.00006\n",
            "and\t1998\t0.00399\n",
            "utter\t14395\t0.00367\n",
            "destruction\t6215\t0.00109\n",
            "our\t2256\t0.26163\n",
            "own\t2219\t0.00001\n",
            "species\t2427\t0.00007\n",
            "has\t2038\t0.10768\n",
            "wrought\t18481\t0.00002\n",
            ",\t1010\t0.00776\n",
            "not\t2025\t0.99552\n",
            "only\t2069\t0.01231\n",
            "upon\t2588\t0.00033\n",
            "animals\t4176\t0.06443\n",
            ",\t1010\t0.90253\n",
            "such\t2107\t0.01417\n",
            "as\t2004\t0.01006\n",
            "the\t1996\t0.77115\n",
            "vanished\t9955\t0.00000\n",
            "bison\t22285\t0.00053\n",
            "and\t1998\t0.00285\n",
            "the\t1996\t0.64182\n",
            "dod\t26489\t0.00890\n",
            "##o\t2080\t0.00034\n",
            ",\t1010\t0.85833\n",
            "but\t2021\t0.00017\n",
            "upon\t2588\t0.01634\n",
            "its\t2049\t0.00060\n",
            "inferior\t14092\t0.00001\n",
            "races\t3837\t0.00004\n",
            ".\t1012\t0.58786\n",
            "accordingly\t11914\t0.00059\n",
            ",\t1010\t0.80888\n",
            "with\t2007\t0.18269\n",
            "such\t2107\t0.01720\n",
            "a\t1037\t0.07792\n",
            "tram\t12517\t0.00651\n",
            "##p\t2361\t0.00001\n",
            "of\t1997\t0.08828\n",
            "his\t2010\t0.30306\n",
            "ponder\t29211\t0.51527\n",
            "##ous\t3560\t0.00003\n",
            "riding\t5559\t0.01672\n",
            "-\t1011\t0.02753\n",
            "boots\t6879\t0.00000\n",
            "as\t2004\t0.00096\n",
            "might\t2453\t0.00895\n",
            "of\t1997\t0.00405\n",
            "itself\t2993\t0.00002\n",
            "have\t2031\t0.01134\n",
            "been\t2042\t0.00027\n",
            "audible\t19525\t0.00001\n",
            "in\t1999\t0.36506\n",
            "the\t1996\t0.97602\n",
            "remote\t6556\t0.00079\n",
            "##st\t3367\t0.00018\n",
            "of\t1997\t0.05508\n",
            "the\t1996\t0.80837\n",
            "seven\t2698\t0.00142\n",
            "gables\t27008\t0.00000\n",
            ",\t1010\t0.25489\n",
            "he\t2002\t0.25192\n",
            "advanced\t3935\t0.00000\n",
            "to\t2000\t0.56990\n",
            "the\t1996\t0.44349\n",
            "door\t2341\t0.21775\n",
            ",\t1010\t0.43810\n",
            "which\t2029\t0.01538\n",
            "the\t1996\t0.03014\n",
            "servant\t7947\t0.00001\n",
            "pointed\t4197\t0.00059\n",
            "out\t2041\t0.00257\n",
            ",\t1010\t0.29141\n",
            "and\t1998\t0.03427\n",
            "made\t2081\t0.00093\n",
            "its\t2049\t0.00181\n",
            "new\t2047\t0.00117\n",
            "panels\t9320\t0.00000\n",
            "re\t2128\t0.00006\n",
            "##ech\t15937\t0.00051\n",
            "##o\t2080\t0.00014\n",
            "with\t2007\t0.62930\n",
            "a\t1037\t0.86652\n",
            "loud\t5189\t0.00281\n",
            ",\t1010\t0.00332\n",
            "free\t2489\t0.00004\n",
            "knock\t7324\t0.00001\n",
            ".\t1012\t0.56262\n",
            "‘\t1520\t0.00000\n",
            "miss\t3335\t0.18143\n",
            "tr\t19817\t0.87336\n",
            "##ot\t4140\t0.00000\n",
            "##wood\t3702\t0.00001\n",
            ",\t1010\t0.08763\n",
            "’\t1521\t0.00000\n",
            "said\t2056\t0.00009\n",
            "the\t1996\t0.06136\n",
            "visitor\t10367\t0.00001\n",
            ".\t1012\t0.94415\n",
            "\"\t1000\t0.99973\n",
            "good\t2204\t0.14343\n",
            "-\t1011\t0.00034\n",
            "morning\t2851\t0.00007\n",
            ",\t1010\t0.92050\n",
            "\"\t1000\t0.19915\n",
            "answered\t4660\t0.00000\n",
            "warwick\t13283\t0.00002\n",
            ".\t1012\t0.96486\n",
            "she\t2016\t0.95379\n",
            "had\t2018\t0.68830\n",
            ",\t1010\t0.05079\n",
            "while\t2096\t0.00002\n",
            "a\t1037\t0.99824\n",
            "very\t2200\t0.00299\n",
            "young\t2402\t0.04570\n",
            "girl\t2611\t0.00012\n",
            ",\t1010\t0.43670\n",
            "as\t2004\t0.97367\n",
            "soon\t2574\t0.00011\n",
            "as\t2004\t0.27670\n",
            "she\t2016\t0.93360\n",
            "had\t2018\t0.96329\n",
            "known\t2124\t0.04201\n",
            "him\t2032\t0.00268\n",
            "to\t2000\t0.91229\n",
            "be\t2022\t0.00035\n",
            ",\t1010\t0.23979\n",
            "in\t1999\t0.92584\n",
            "the\t1996\t0.94428\n",
            "event\t2724\t0.00008\n",
            "of\t1997\t0.12187\n",
            "her\t2014\t0.03172\n",
            "having\t2383\t0.00235\n",
            "no\t2053\t0.00050\n",
            "brother\t2567\t0.00209\n",
            ",\t1010\t0.50002\n",
            "the\t1996\t0.46990\n",
            "future\t2925\t0.00049\n",
            "baronet\t8693\t0.00019\n",
            ",\t1010\t0.00001\n",
            "meant\t3214\t0.06173\n",
            "to\t2000\t0.88932\n",
            "marry\t5914\t0.00705\n",
            "him\t2032\t0.00120\n",
            ",\t1010\t0.68070\n",
            "and\t1998\t0.04804\n",
            "her\t2014\t0.97180\n",
            "father\t2269\t0.00006\n",
            "had\t2018\t0.96156\n",
            "always\t2467\t0.11247\n",
            "meant\t3214\t0.00386\n",
            "that\t2008\t0.32503\n",
            "she\t2016\t0.03349\n",
            "should\t2323\t0.00003\n",
            ".\t1012\t0.59129\n",
            "there\t2045\t0.99442\n",
            "were\t2020\t0.00207\n",
            "mothers\t10756\t0.02328\n",
            "and\t1998\t0.00241\n",
            "brothers\t3428\t0.97830\n",
            "and\t1998\t0.06307\n",
            "sisters\t5208\t0.01212\n",
            ",\t1010\t0.00055\n",
            "and\t1998\t0.00996\n",
            "aunt\t5916\t0.54403\n",
            "##s\t2015\t0.08404\n",
            "and\t1998\t0.02164\n",
            "cousins\t12334\t0.00001\n",
            "to\t2000\t0.05204\n",
            "express\t4671\t0.00016\n",
            "various\t2536\t0.00058\n",
            "opinions\t10740\t0.00076\n",
            "on\t2006\t0.52392\n",
            "the\t1996\t0.63589\n",
            "subject\t3395\t0.00001\n",
            ",\t1010\t0.65197\n",
            "but\t2021\t0.00143\n",
            "as\t2004\t0.00720\n",
            "to\t2000\t0.04113\n",
            "what\t2054\t0.00123\n",
            "they\t2027\t0.00025\n",
            "several\t2195\t0.00001\n",
            "##ly\t2135\t0.02930\n",
            "advised\t9449\t0.00000\n",
            "history\t2381\t0.00081\n",
            "is\t2003\t0.10007\n",
            "silent\t4333\t0.00003\n",
            ".\t1012\t0.67746\n",
            "her\t2014\t0.31723\n",
            "sen\t12411\t0.99979\n",
            "##sibility\t28255\t0.00000\n",
            "prompted\t9469\t0.02511\n",
            "her\t2014\t0.00028\n",
            "to\t2000\t0.35023\n",
            "search\t3945\t0.01215\n",
            "for\t2005\t0.01509\n",
            "an\t2019\t0.83344\n",
            "object\t4874\t0.00141\n",
            "to\t2000\t0.02851\n",
            "love\t2293\t0.00598\n",
            ";\t1025\t0.00009\n",
            "on\t2006\t0.59837\n",
            "earth\t3011\t0.00000\n",
            "it\t2009\t0.02808\n",
            "was\t2001\t0.84607\n",
            "not\t2025\t0.00720\n",
            "to\t2000\t0.89043\n",
            "be\t2022\t0.00823\n",
            "found\t2179\t0.00004\n",
            ":\t1024\t0.01463\n",
            "her\t2014\t0.94593\n",
            "mother\t2388\t0.00118\n",
            "had\t2018\t0.61266\n",
            "often\t2411\t0.02297\n",
            "disappointed\t9364\t0.00001\n",
            "her\t2014\t0.00551\n",
            ",\t1010\t0.82329\n",
            "and\t1998\t0.12052\n",
            "the\t1996\t0.86139\n",
            "apparent\t6835\t0.00001\n",
            "partial\t7704\t0.00690\n",
            "##ity\t3012\t0.00000\n",
            "she\t2016\t0.21763\n",
            "she\t2016\t0.10882\n",
            "##wed\t15557\t0.00003\n",
            "to\t2000\t0.00708\n",
            "her\t2014\t0.00331\n",
            "brother\t2567\t0.00001\n",
            "gave\t2435\t0.00212\n",
            "her\t2014\t0.00725\n",
            "exquisite\t19401\t0.00239\n",
            "pain\t3255\t0.00009\n",
            "-\t1011\t0.01227\n",
            "-\t1011\t0.00794\n",
            "produced\t2550\t0.00280\n",
            "a\t1037\t0.88561\n",
            "kind\t2785\t0.05263\n",
            "of\t1997\t0.27021\n",
            "habit\t10427\t0.41530\n",
            "##ual\t8787\t0.00007\n",
            "melancholy\t22247\t0.00005\n",
            ",\t1010\t0.02874\n",
            "led\t2419\t0.07908\n",
            "her\t2014\t0.00393\n",
            "into\t2046\t0.00156\n",
            "a\t1037\t0.02283\n",
            "fond\t13545\t0.24856\n",
            "##ness\t2791\t0.00002\n",
            "for\t2005\t0.00212\n",
            "reading\t3752\t0.00001\n",
            "tales\t7122\t0.00015\n",
            "of\t1997\t0.00272\n",
            "wo\t24185\t0.00063\n",
            "##e\t2063\t0.01104\n",
            ",\t1010\t0.20466\n",
            "and\t1998\t0.05639\n",
            "made\t2081\t0.26716\n",
            "her\t2014\t0.00017\n",
            "almost\t2471\t0.00030\n",
            "realize\t5382\t0.00001\n",
            "the\t1996\t0.10937\n",
            "fictitious\t23577\t0.00000\n",
            "distress\t12893\t0.00000\n",
            ".\t1012\t0.52768\n",
            "“\t1523\t0.00000\n",
            "and\t1998\t0.28523\n",
            "so\t2061\t0.00591\n",
            "this\t2023\t0.74802\n",
            "is\t2003\t0.26794\n",
            "your\t2115\t0.00077\n",
            "first\t2034\t0.11825\n",
            "visit\t3942\t0.00035\n",
            "to\t2000\t0.00398\n",
            "chicago\t3190\t0.00001\n",
            ",\t1010\t0.02033\n",
            "”\t1524\t0.00000\n",
            "he\t2002\t0.00089\n",
            "observed\t5159\t0.00000\n",
            ".\t1012\t0.91250\n",
            "\"\t1000\t0.99872\n",
            "for\t2005\t0.02739\n",
            "whom\t3183\t0.00388\n",
            "is\t2003\t0.74626\n",
            "this\t2023\t0.00384\n",
            ",\t1010\t0.66135\n",
            "miss\t3335\t0.11344\n",
            "jem\t24193\t0.20789\n",
            "##ima\t9581\t0.00001\n",
            "?\t1029\t0.04080\n",
            "\"\t1000\t0.52067\n",
            "said\t2056\t0.00609\n",
            "miss\t3335\t0.01550\n",
            "pink\t5061\t0.51076\n",
            "##erton\t20995\t0.00003\n",
            ",\t1010\t0.18093\n",
            "with\t2007\t0.00358\n",
            "awful\t9643\t0.00006\n",
            "cold\t3147\t0.02660\n",
            "##ness\t2791\t0.00005\n",
            ".\t1012\t0.84720\n",
            "they\t2027\t0.85436\n",
            "th\t16215\t0.99993\n",
            "##rong\t17583\t0.00030\n",
            "##ed\t2098\t0.00022\n",
            ",\t1010\t0.99648\n",
            "however\t2174\t0.00006\n",
            ",\t1010\t0.00526\n",
            "to\t2000\t0.22726\n",
            "the\t1996\t0.79316\n",
            "now\t2085\t0.00000\n",
            "open\t2330\t0.00007\n",
            "door\t2341\t0.00067\n",
            ",\t1010\t0.44925\n",
            "pressing\t7827\t0.00002\n",
            "the\t1996\t0.94823\n",
            "lieutenant\t3812\t0.50285\n",
            "-\t1011\t0.21211\n",
            "governor\t3099\t0.00000\n",
            ",\t1010\t0.94881\n",
            "in\t1999\t0.13858\n",
            "the\t1996\t0.87043\n",
            "eager\t9461\t0.04593\n",
            "##ness\t2791\t0.00041\n",
            "of\t1997\t0.14952\n",
            "their\t2037\t0.01386\n",
            "curiosity\t10628\t0.00008\n",
            ",\t1010\t0.08555\n",
            "into\t2046\t0.20894\n",
            "the\t1996\t0.82929\n",
            "room\t2282\t0.00080\n",
            "in\t1999\t0.28682\n",
            "advance\t5083\t0.00001\n",
            "of\t1997\t0.06131\n",
            "them\t2068\t0.00029\n",
            ".\t1012\t0.63382\n",
            "his\t2010\t0.22076\n",
            "remark\t17674\t0.00251\n",
            "did\t2106\t0.88389\n",
            "not\t2025\t0.47934\n",
            "seem\t4025\t0.00024\n",
            "at\t2012\t0.01194\n",
            "all\t2035\t0.00127\n",
            "surprising\t11341\t0.00000\n",
            ".\t1012\t0.96486\n",
            "“\t1523\t0.00000\n",
            "and\t1998\t0.08972\n",
            "are\t2024\t0.04443\n",
            "american\t2137\t0.00024\n",
            "little\t2210\t0.68509\n",
            "boys\t3337\t0.00063\n",
            "the\t1996\t0.53851\n",
            "best\t2190\t0.00054\n",
            "little\t2210\t0.03809\n",
            "boys\t3337\t0.00092\n",
            "?\t1029\t0.00042\n",
            "”\t1524\t0.00000\n",
            "so\t2061\t0.00020\n",
            "with\t2007\t0.01535\n",
            "other\t2060\t0.00409\n",
            "vanishing\t24866\t0.00000\n",
            "##s\t2015\t0.01095\n",
            ".\t1012\t0.98295\n",
            "the\t1996\t0.12260\n",
            "search\t3945\t0.00085\n",
            "was\t2001\t0.93895\n",
            "made\t2081\t0.00015\n",
            ",\t1010\t0.64928\n",
            "and\t1998\t0.01498\n",
            "it\t2009\t0.03375\n",
            "ended\t3092\t0.00000\n",
            "-\t1011\t0.08328\n",
            "-\t1011\t0.01693\n",
            "in\t1999\t0.00758\n",
            "william\t2520\t0.00008\n",
            "dane\t14569\t0.00001\n",
            "'\t1005\t0.99884\n",
            "s\t1055\t0.00004\n",
            "finding\t4531\t0.00026\n",
            "the\t1996\t0.43121\n",
            "well\t2092\t0.51628\n",
            "-\t1011\t0.00990\n",
            "known\t2124\t0.00001\n",
            "bag\t4524\t0.01763\n",
            ",\t1010\t0.18327\n",
            "empty\t4064\t0.00005\n",
            ",\t1010\t0.04765\n",
            "tucked\t9332\t0.00040\n",
            "behind\t2369\t0.00331\n",
            "the\t1996\t0.74474\n",
            "chest\t3108\t0.96020\n",
            "of\t1997\t0.04940\n",
            "drawers\t22497\t0.00000\n",
            "in\t1999\t0.34724\n",
            "silas\t18553\t0.00001\n",
            "'\t1005\t0.99954\n",
            "s\t1055\t0.00010\n",
            "chamber\t4574\t0.00001\n",
            "!\t999\t0.04114\n",
            "there\t2045\t0.99976\n",
            "were\t2020\t0.78882\n",
            "doors\t4303\t0.00005\n",
            "all\t2035\t0.44005\n",
            "round\t2461\t0.00023\n",
            "the\t1996\t0.93131\n",
            "hall\t2534\t0.00008\n",
            ",\t1010\t0.89956\n",
            "but\t2021\t0.00987\n",
            "they\t2027\t0.00361\n",
            "were\t2020\t0.74574\n",
            "all\t2035\t0.01104\n",
            "locked\t5299\t0.00057\n",
            ";\t1025\t0.01046\n",
            "and\t1998\t0.03120\n",
            "when\t2043\t0.30782\n",
            "alice\t5650\t0.00044\n",
            "had\t2018\t0.73420\n",
            "been\t2042\t0.00000\n",
            "all\t2035\t0.98286\n",
            "the\t1996\t0.01718\n",
            "way\t2126\t0.00081\n",
            "down\t2091\t0.48939\n",
            "one\t2028\t0.98087\n",
            "side\t2217\t0.00045\n",
            "and\t1998\t0.02611\n",
            "up\t2039\t0.03174\n",
            "the\t1996\t0.04566\n",
            "other\t2060\t0.00002\n",
            ",\t1010\t0.04601\n",
            "trying\t2667\t0.00016\n",
            "every\t2296\t0.00010\n",
            "door\t2341\t0.00092\n",
            ",\t1010\t0.06437\n",
            "she\t2016\t0.67037\n",
            "walked\t2939\t0.02253\n",
            "sadly\t13718\t0.00033\n",
            "down\t2091\t0.19669\n",
            "the\t1996\t0.93889\n",
            "middle\t2690\t0.00000\n",
            ",\t1010\t0.94774\n",
            "wondering\t6603\t0.12067\n",
            "how\t2129\t0.00009\n",
            "she\t2016\t0.03604\n",
            "was\t2001\t0.02751\n",
            "ever\t2412\t0.00057\n",
            "to\t2000\t0.16507\n",
            "get\t2131\t0.00055\n",
            "out\t2041\t0.02003\n",
            "again\t2153\t0.00019\n",
            ".\t1012\t0.55374\n",
            "ogden\t23203\t0.00001\n",
            "ford\t4811\t0.00001\n",
            "was\t2001\t0.84918\n",
            "sprawling\t24199\t0.00022\n",
            "in\t1999\t0.35729\n",
            "a\t1037\t0.74569\n",
            "deep\t2784\t0.00000\n",
            "chair\t3242\t0.00000\n",
            "in\t1999\t0.04333\n",
            "the\t1996\t0.05506\n",
            "shadows\t6281\t0.00001\n",
            ".\t1012\t0.94134\n",
            "the\t1996\t0.56402\n",
            "school\t2082\t0.64977\n",
            "##master\t8706\t0.00041\n",
            "is\t2003\t0.06330\n",
            "generally\t3227\t0.00016\n",
            "a\t1037\t0.92450\n",
            "man\t2158\t0.01124\n",
            "of\t1997\t0.18112\n",
            "some\t2070\t0.02741\n",
            "importance\t5197\t0.00018\n",
            "in\t1999\t0.52951\n",
            "the\t1996\t0.95296\n",
            "female\t2931\t0.00001\n",
            "circle\t4418\t0.00001\n",
            "of\t1997\t0.15914\n",
            "a\t1037\t0.51360\n",
            "rural\t3541\t0.00045\n",
            "neighborhood\t5101\t0.00001\n",
            ";\t1025\t0.00043\n",
            "being\t2108\t0.00955\n",
            "considered\t2641\t0.01211\n",
            "a\t1037\t0.87132\n",
            "kind\t2785\t0.03392\n",
            "of\t1997\t0.06611\n",
            "idle\t18373\t0.00002\n",
            ",\t1010\t0.03906\n",
            "gentleman\t10170\t0.00640\n",
            "##like\t10359\t0.00064\n",
            "persona\t16115\t0.34402\n",
            "##ge\t3351\t0.00014\n",
            ",\t1010\t0.00228\n",
            "of\t1997\t0.34723\n",
            "vastly\t24821\t0.00010\n",
            "superior\t6020\t0.00044\n",
            "taste\t5510\t0.00033\n",
            "and\t1998\t0.01051\n",
            "accomplishments\t17571\t0.00000\n",
            "to\t2000\t0.05303\n",
            "the\t1996\t0.64618\n",
            "rough\t5931\t0.00031\n",
            "country\t2406\t0.00001\n",
            "sw\t25430\t0.07230\n",
            "##ains\t28247\t0.00002\n",
            ",\t1010\t0.48055\n",
            "and\t1998\t0.22670\n",
            ",\t1010\t0.14151\n",
            "indeed\t5262\t0.00004\n",
            ",\t1010\t0.03592\n",
            "inferior\t14092\t0.00916\n",
            "in\t1999\t0.01014\n",
            "learning\t4083\t0.00000\n",
            "only\t2069\t0.01368\n",
            "to\t2000\t0.00692\n",
            "the\t1996\t0.09385\n",
            "par\t11968\t0.01503\n",
            "##son\t3385\t0.00052\n",
            ".\t1012\t0.56262\n",
            "\"\t1000\t0.00121\n",
            "my\t2026\t0.54170\n",
            "friend\t2767\t0.00832\n",
            ",\t1010\t0.10051\n",
            "forget\t5293\t0.00102\n",
            "your\t2115\t0.61734\n",
            "resentment\t20234\t0.00000\n",
            ",\t1010\t0.02976\n",
            "in\t1999\t0.32248\n",
            "favour\t7927\t0.00001\n",
            "of\t1997\t0.52118\n",
            "your\t2115\t0.00022\n",
            "humanity\t8438\t0.00003\n",
            ";\t1025\t0.00016\n",
            "-\t1011\t0.00378\n",
            "a\t1037\t0.02633\n",
            "father\t2269\t0.06230\n",
            ",\t1010\t0.52215\n",
            "trembling\t10226\t0.00000\n",
            "for\t2005\t0.23026\n",
            "the\t1996\t0.99802\n",
            "welfare\t7574\t0.00124\n",
            "of\t1997\t0.08389\n",
            "his\t2010\t0.00413\n",
            "child\t2775\t0.00199\n",
            ",\t1010\t0.00153\n",
            "be\t2022\t0.99935\n",
            "##que\t4226\t0.00135\n",
            "##ath\t8988\t0.00000\n",
            "##s\t2015\t0.00007\n",
            "her\t2014\t0.00468\n",
            "to\t2000\t0.04665\n",
            "your\t2115\t0.00115\n",
            "care\t2729\t0.00005\n",
            ".\t1012\t0.63382\n",
            "he\t2002\t0.82822\n",
            "was\t2001\t0.84155\n",
            "a\t1037\t0.99600\n",
            "very\t2200\t0.00006\n",
            "silent\t4333\t0.00002\n",
            "man\t2158\t0.00001\n",
            "by\t2011\t0.02263\n",
            "custom\t7661\t0.00000\n",
            ".\t1012\t0.96486\n",
            "“\t1523\t0.00000\n",
            "i\t1045\t0.78460\n",
            "never\t2196\t0.00288\n",
            "did\t2106\t0.00339\n",
            "see\t2156\t0.00044\n",
            "the\t1996\t0.25918\n",
            "beat\t3786\t0.00006\n",
            "of\t1997\t0.00801\n",
            "that\t2008\t0.02238\n",
            "boy\t2879\t0.00539\n",
            "!\t999\t0.00029\n",
            "”\t1524\t0.00000\n",
            "\"\t1000\t0.07020\n",
            "mercy\t8673\t0.02607\n",
            "on\t2006\t0.00223\n",
            "us\t2149\t0.00784\n",
            ",\t1010\t0.64830\n",
            "good\t2204\t0.07389\n",
            "##wife\t19993\t0.00004\n",
            "!\t999\t0.77871\n",
            "\"\t1000\t0.86681\n",
            "exclaimed\t12713\t0.00003\n",
            "a\t1037\t0.08967\n",
            "man\t2158\t0.00028\n",
            "in\t1999\t0.00931\n",
            "the\t1996\t0.95334\n",
            "crowd\t4306\t0.00001\n",
            ",\t1010\t0.45502\n",
            "\"\t1000\t0.03453\n",
            "is\t2003\t0.97608\n",
            "there\t2045\t0.03047\n",
            "no\t2053\t0.10233\n",
            "virtue\t11870\t0.00129\n",
            "in\t1999\t0.00015\n",
            "woman\t2450\t0.00772\n",
            ",\t1010\t0.43205\n",
            "save\t3828\t0.00004\n",
            "what\t2054\t0.01551\n",
            "springs\t6076\t0.00005\n",
            "from\t2013\t0.00425\n",
            "a\t1037\t0.09164\n",
            "whole\t2878\t0.48601\n",
            "##some\t14045\t0.00003\n",
            "fear\t3571\t0.00053\n",
            "of\t1997\t0.00619\n",
            "the\t1996\t0.88166\n",
            "gallo\t25624\t0.30278\n",
            "##ws\t9333\t0.00002\n",
            "?\t1029\t0.01813\n",
            "that\t2008\t0.07066\n",
            "is\t2003\t0.31362\n",
            "the\t1996\t0.77883\n",
            "hardest\t18263\t0.00000\n",
            "word\t2773\t0.00002\n",
            "yet\t2664\t0.00008\n",
            "!\t999\t0.06287\n",
            "rt\t19387\t0.00001\n",
            "@\t1030\t0.00004\n",
            "white\t2317\t0.02220\n",
            "##house\t4580\t0.00002\n",
            ":\t1024\t0.40911\n",
            "since\t2144\t0.03638\n",
            "august\t2257\t0.11093\n",
            "14\t2403\t0.00000\n",
            ",\t1010\t0.06084\n",
            "the\t1996\t0.99818\n",
            "u\t1057\t0.99642\n",
            ".\t1012\t0.29261\n",
            "s\t1055\t0.00038\n",
            ".\t1012\t0.00101\n",
            "has\t2038\t0.00030\n",
            "evacuated\t13377\t0.00023\n",
            "and\t1998\t0.03893\n",
            "facilitated\t19601\t0.00085\n",
            "the\t1996\t0.90025\n",
            "evacuation\t13982\t0.00001\n",
            "of\t1997\t0.04626\n",
            "approximately\t3155\t0.00692\n",
            "116\t12904\t0.00014\n",
            ",\t1010\t0.07106\n",
            "700\t6352\t0.00002\n",
            "people\t2111\t0.00084\n",
            ".\t1012\t0.75792\n",
            "rt\t19387\t0.00004\n",
            "@\t1030\t0.00101\n",
            "white\t2317\t0.03679\n",
            "##house\t4580\t0.00000\n",
            ":\t1024\t0.11325\n",
            "from\t2013\t0.66014\n",
            "3\t1017\t0.01381\n",
            "am\t2572\t0.00969\n",
            "et\t3802\t0.11176\n",
            "on\t2006\t0.27833\n",
            "8\t1022\t0.22168\n",
            "/\t1013\t0.11817\n",
            "29\t2756\t0.00012\n",
            "to\t2000\t0.02253\n",
            "3a\t23842\t0.00564\n",
            "##m\t2213\t0.00100\n",
            "et\t3802\t0.00000\n",
            "on\t2006\t0.72397\n",
            "8\t1022\t0.00005\n",
            "/\t1013\t0.00001\n",
            "30\t2382\t0.00000\n",
            ",\t1010\t0.06993\n",
            "a\t1037\t0.99921\n",
            "total\t2561\t0.77244\n",
            "of\t1997\t0.02044\n",
            "approximately\t3155\t0.07203\n",
            "1\t1015\t0.52680\n",
            ",\t1010\t0.17990\n",
            "200\t3263\t0.08879\n",
            "people\t2111\t0.02812\n",
            "were\t2020\t0.27020\n",
            "evacuated\t13377\t0.00060\n",
            "from\t2013\t0.00752\n",
            "kabul\t21073\t0.00000\n",
            ".\t1012\t0.63382\n",
            "thanks\t4283\t0.80995\n",
            "to\t2000\t0.06070\n",
            "the\t1996\t0.99321\n",
            "hard\t2524\t0.00003\n",
            "work\t2147\t0.00146\n",
            "of\t1997\t0.00139\n",
            "@\t1030\t0.00020\n",
            "fe\t10768\t0.96356\n",
            "##ma\t2863\t0.00000\n",
            ",\t1010\t0.04891\n",
            "we\t2057\t0.07931\n",
            "’\t1521\t0.00000\n",
            "ve\t2310\t0.00001\n",
            "pre\t3653\t0.09132\n",
            "-\t1011\t0.01038\n",
            "positioned\t10959\t0.00001\n",
            "resources\t4219\t0.00637\n",
            ",\t1010\t0.17040\n",
            "equipment\t3941\t0.00083\n",
            ",\t1010\t0.00222\n",
            "and\t1998\t0.04929\n",
            "response\t3433\t0.00682\n",
            "teams\t2780\t0.00019\n",
            "to\t2000\t0.32566\n",
            "respond\t6869\t0.00008\n",
            "to\t2000\t0.01108\n",
            "hurricane\t7064\t0.00004\n",
            "ida\t16096\t0.00002\n",
            ".\t1012\t0.71784\n",
            "our\t2256\t0.01828\n",
            "whole\t2878\t0.00264\n",
            "-\t1011\t0.69612\n",
            "of\t1997\t0.00010\n",
            "-\t1011\t0.00197\n",
            "government\t2231\t0.00004\n",
            "effort\t3947\t0.00000\n",
            "is\t2003\t0.01936\n",
            "already\t2525\t0.00168\n",
            "hard\t2524\t0.00234\n",
            "at\t2012\t0.01627\n",
            "work\t2147\t0.00000\n",
            ".\t1012\t0.92896\n",
            "the\t1996\t0.93174\n",
            "13\t2410\t0.00001\n",
            "service\t2326\t0.00218\n",
            "members\t2372\t0.00079\n",
            "that\t2008\t0.04088\n",
            "we\t2057\t0.16104\n",
            "lost\t2439\t0.00003\n",
            "were\t2020\t0.00021\n",
            "heroes\t7348\t0.00192\n",
            "who\t2040\t0.00766\n",
            "made\t2081\t0.05665\n",
            "the\t1996\t0.85824\n",
            "ultimate\t7209\t0.00426\n",
            "sacrifice\t8688\t0.00739\n",
            "in\t1999\t0.15445\n",
            "service\t2326\t0.00040\n",
            "of\t1997\t0.02220\n",
            "our\t2256\t0.00099\n",
            "highest\t3284\t0.00007\n",
            "american\t2137\t0.02174\n",
            "ideals\t15084\t0.00001\n",
            "and\t1998\t0.00152\n",
            "while\t2096\t0.00912\n",
            "saving\t7494\t0.00020\n",
            "the\t1996\t0.99820\n",
            "lives\t3268\t0.00437\n",
            "of\t1997\t0.00045\n",
            "others\t2500\t0.00055\n",
            ".\t1012\t0.71784\n",
            "to\t2000\t0.40389\n",
            "the\t1996\t0.10996\n",
            "people\t2111\t0.04927\n",
            "of\t1997\t0.20346\n",
            "the\t1996\t0.58162\n",
            "gulf\t6084\t0.02651\n",
            "coast\t3023\t0.00001\n",
            ":\t1024\t0.00986\n",
            "please\t3531\t0.00118\n",
            "follow\t3582\t0.01092\n",
            "the\t1996\t0.89776\n",
            "instructions\t8128\t0.00009\n",
            "of\t1997\t0.04625\n",
            "local\t2334\t0.00592\n",
            "officials\t4584\t0.00001\n",
            "during\t2076\t0.02274\n",
            "this\t2023\t0.37333\n",
            "dangerous\t4795\t0.00000\n",
            "time\t2051\t0.00005\n",
            ".\t1012\t0.86914\n",
            "today\t2651\t0.00789\n",
            "i\t1045\t0.79816\n",
            "was\t2001\t0.60537\n",
            "brief\t4766\t0.70660\n",
            "##ed\t2098\t0.00001\n",
            "on\t2006\t0.63828\n",
            "our\t2256\t0.00014\n",
            "preparations\t12929\t0.00030\n",
            "for\t2005\t0.08663\n",
            "hurricane\t7064\t0.00002\n",
            "ida\t16096\t0.00000\n",
            "by\t2011\t0.00136\n",
            "@\t1030\t0.00048\n",
            "fe\t10768\t0.00170\n",
            "##ma\t2863\t0.00003\n",
            ".\t1012\t0.90057\n",
            "i\t1045\t0.22802\n",
            "said\t2056\t0.03394\n",
            "we\t2057\t0.48919\n",
            "would\t2052\t0.04634\n",
            "go\t2175\t0.04003\n",
            "after\t2044\t0.00009\n",
            "the\t1996\t0.41234\n",
            "group\t2177\t0.00000\n",
            "responsible\t3625\t0.04247\n",
            "for\t2005\t0.02589\n",
            "the\t1996\t0.05236\n",
            "attack\t2886\t0.02043\n",
            "on\t2006\t0.16431\n",
            "our\t2256\t0.07216\n",
            "troops\t3629\t0.00084\n",
            "and\t1998\t0.00006\n",
            "innocent\t7036\t0.01326\n",
            "civilians\t9272\t0.00002\n",
            "in\t1999\t0.71557\n",
            "kabul\t21073\t0.00000\n",
            ",\t1010\t0.45023\n",
            "and\t1998\t0.03901\n",
            "we\t2057\t0.00033\n",
            "have\t2031\t0.00020\n",
            ".\t1012\t0.82102\n",
            "rt\t19387\t0.00001\n",
            "@\t1030\t0.00004\n",
            "white\t2317\t0.02474\n",
            "##house\t4580\t0.00001\n",
            ":\t1024\t0.39819\n",
            "since\t2144\t0.03609\n",
            "august\t2257\t0.11073\n",
            "14\t2403\t0.00000\n",
            ",\t1010\t0.07681\n",
            "the\t1996\t0.99776\n",
            "u\t1057\t0.99597\n",
            ".\t1012\t0.26743\n",
            "s\t1055\t0.00035\n",
            ".\t1012\t0.00141\n",
            "has\t2038\t0.00036\n",
            "evacuated\t13377\t0.00034\n",
            "and\t1998\t0.05374\n",
            "facilitated\t19601\t0.00065\n",
            "the\t1996\t0.88452\n",
            "evacuation\t13982\t0.00000\n",
            "of\t1997\t0.04791\n",
            "approximately\t3155\t0.00363\n",
            "111\t11118\t0.00013\n",
            ",\t1010\t0.05171\n",
            "900\t7706\t0.00001\n",
            "people\t2111\t0.00084\n",
            ".\t1012\t0.75792\n",
            "rt\t19387\t0.00010\n",
            "@\t1030\t0.00243\n",
            "white\t2317\t0.17655\n",
            "##house\t4580\t0.00000\n",
            ":\t1024\t0.05811\n",
            "update\t10651\t0.00097\n",
            ":\t1024\t0.09644\n",
            "from\t2013\t0.50341\n",
            "3\t1017\t0.04914\n",
            "am\t2572\t0.14427\n",
            "et\t3802\t0.00048\n",
            "on\t2006\t0.13808\n",
            "8\t1022\t0.35955\n",
            "/\t1013\t0.00068\n",
            "27\t2676\t0.00237\n",
            "to\t2000\t0.03429\n",
            "3\t1017\t0.02066\n",
            "am\t2572\t0.00035\n",
            "et\t3802\t0.00001\n",
            "on\t2006\t0.82286\n",
            "8\t1022\t0.00004\n",
            "/\t1013\t0.00002\n",
            "28\t2654\t0.00000\n",
            ",\t1010\t0.12520\n",
            "a\t1037\t0.99949\n",
            "total\t2561\t0.67786\n",
            "of\t1997\t0.02279\n",
            "approximately\t3155\t0.08297\n",
            "6\t1020\t0.03488\n",
            ",\t1010\t0.24352\n",
            "800\t5385\t0.01469\n",
            "people\t2111\t0.02265\n",
            "were\t2020\t0.09122\n",
            "evacuated\t13377\t0.00055\n",
            "from\t2013\t0.00522\n",
            "kabul\t21073\t0.00000\n",
            ".\t1012\t0.67045\n",
            "this\t2023\t0.12108\n",
            "afternoon\t5027\t0.06504\n",
            ",\t1010\t0.95916\n",
            "i\t1045\t0.00048\n",
            "held\t2218\t0.03391\n",
            "a\t1037\t0.97655\n",
            "call\t2655\t0.00066\n",
            "with\t2007\t0.01208\n",
            "the\t1996\t0.78311\n",
            "head\t2132\t0.05749\n",
            "of\t1997\t0.00501\n",
            "fe\t10768\t0.04339\n",
            "##ma\t2863\t0.00000\n",
            "and\t1998\t0.00388\n",
            "governors\t11141\t0.00000\n",
            "ahead\t3805\t0.00195\n",
            "of\t1997\t0.00616\n",
            "hurricane\t7064\t0.00005\n",
            "ida\t16096\t0.00003\n",
            "to\t2000\t0.18212\n",
            "discuss\t6848\t0.00002\n",
            "preparations\t12929\t0.00899\n",
            "for\t2005\t0.10230\n",
            "what\t2054\t0.00045\n",
            "is\t2003\t0.01945\n",
            "expected\t3517\t0.00114\n",
            "to\t2000\t0.16752\n",
            "be\t2022\t0.00057\n",
            "a\t1037\t0.62786\n",
            "dangerous\t4795\t0.00002\n",
            "storm\t4040\t0.00003\n",
            ".\t1012\t0.68000\n",
            "it\t2009\t0.99474\n",
            "was\t2001\t0.31811\n",
            "an\t2019\t0.13006\n",
            "honor\t3932\t0.00017\n",
            "to\t2000\t0.06710\n",
            "welcome\t6160\t0.00077\n",
            "israeli\t5611\t0.25186\n",
            "prime\t3539\t0.75745\n",
            "minister\t2704\t0.00070\n",
            "na\t6583\t0.98772\n",
            "##ft\t6199\t0.00000\n",
            "##ali\t11475\t0.00000\n",
            "bennett\t8076\t0.00001\n",
            "to\t2000\t0.01658\n",
            "the\t1996\t0.99242\n",
            "white\t2317\t0.00166\n",
            "house\t2160\t0.00197\n",
            "today\t2651\t0.00003\n",
            ".\t1012\t0.86708\n",
            "rt\t19387\t0.00001\n",
            "@\t1030\t0.00004\n",
            "white\t2317\t0.02477\n",
            "##house\t4580\t0.00002\n",
            ":\t1024\t0.42396\n",
            "since\t2144\t0.04239\n",
            "august\t2257\t0.10874\n",
            "14\t2403\t0.00000\n",
            ",\t1010\t0.11311\n",
            "the\t1996\t0.99846\n",
            "u\t1057\t0.99536\n",
            ".\t1012\t0.27526\n",
            "s\t1055\t0.00026\n",
            ".\t1012\t0.00125\n",
            "has\t2038\t0.00045\n",
            "evacuated\t13377\t0.00011\n",
            "and\t1998\t0.04693\n",
            "facilitated\t19601\t0.00071\n",
            "the\t1996\t0.91416\n",
            "evacuation\t13982\t0.00001\n",
            "of\t1997\t0.02919\n",
            "approximately\t3155\t0.10103\n",
            "105\t8746\t0.00055\n",
            ",\t1010\t0.85702\n",
            "000\t2199\t0.00006\n",
            "people\t2111\t0.00084\n",
            ".\t1012\t0.75792\n",
            "rt\t19387\t0.00012\n",
            "@\t1030\t0.00229\n",
            "white\t2317\t0.18789\n",
            "##house\t4580\t0.00000\n",
            ":\t1024\t0.05015\n",
            "update\t10651\t0.00222\n",
            ":\t1024\t0.04631\n",
            "from\t2013\t0.15495\n",
            "3\t1017\t0.05386\n",
            "am\t2572\t0.19495\n",
            "et\t3802\t0.00067\n",
            "on\t2006\t0.22199\n",
            "8\t1022\t0.10917\n",
            "/\t1013\t0.00124\n",
            "26\t2656\t0.00167\n",
            "to\t2000\t0.02647\n",
            "3\t1017\t0.02348\n",
            "am\t2572\t0.00036\n",
            "et\t3802\t0.00002\n",
            "on\t2006\t0.09441\n",
            "8\t1022\t0.00043\n",
            "/\t1013\t0.00019\n",
            "27\t2676\t0.00000\n",
            "a\t1037\t0.99940\n",
            "total\t2561\t0.79285\n",
            "of\t1997\t0.01490\n",
            "approximately\t3155\t0.09848\n",
            "12\t2260\t0.00859\n",
            ",\t1010\t0.38871\n",
            "500\t3156\t0.09564\n",
            "people\t2111\t0.02675\n",
            "were\t2020\t0.13111\n",
            "evacuated\t13377\t0.00055\n",
            "from\t2013\t0.00961\n",
            "kabul\t21073\t0.00000\n",
            ".\t1012\t0.67087\n",
            "the\t1996\t0.27547\n",
            "american\t2137\t0.03126\n",
            "service\t2326\t0.00481\n",
            "members\t2372\t0.00219\n",
            "who\t2040\t0.93223\n",
            "gave\t2435\t0.00137\n",
            "their\t2037\t0.79388\n",
            "lives\t3268\t0.00009\n",
            "were\t2020\t0.00023\n",
            "heroes\t7348\t0.00000\n",
            ".\t1012\t0.94415\n",
            "watch\t3422\t0.11663\n",
            "as\t2004\t0.00244\n",
            "i\t1045\t0.00007\n",
            "deliver\t8116\t0.00077\n",
            "remarks\t12629\t0.00054\n",
            "on\t2006\t0.17427\n",
            "the\t1996\t0.32216\n",
            "terror\t7404\t0.12296\n",
            "attack\t2886\t0.00008\n",
            "at\t2012\t0.45485\n",
            "hamid\t24811\t0.13588\n",
            "ka\t10556\t0.99591\n",
            "##rza\t24175\t0.00000\n",
            "##i\t2072\t0.00221\n",
            "international\t2248\t0.65429\n",
            "airport\t3199\t0.00001\n",
            ",\t1010\t0.24105\n",
            "and\t1998\t0.02387\n",
            "the\t1996\t0.00169\n",
            "u\t1057\t0.99765\n",
            ".\t1012\t0.25145\n",
            "s\t1055\t0.00111\n",
            ".\t1012\t0.00223\n",
            "service\t2326\t0.00732\n",
            "members\t2372\t0.00005\n",
            "and\t1998\t0.00686\n",
            "afghan\t12632\t0.00002\n",
            "victims\t5694\t0.00005\n",
            "killed\t2730\t0.05384\n",
            "and\t1998\t0.02107\n",
            "wounded\t5303\t0.00001\n",
            ".\t1012\t0.68000\n",
            "rt\t19387\t0.00001\n",
            "@\t1030\t0.00004\n",
            "white\t2317\t0.02093\n",
            "##house\t4580\t0.00002\n",
            ":\t1024\t0.41338\n",
            "since\t2144\t0.03094\n",
            "august\t2257\t0.11018\n",
            "14\t2403\t0.00000\n",
            ",\t1010\t0.04065\n",
            "the\t1996\t0.99810\n",
            "u\t1057\t0.99681\n",
            ".\t1012\t0.29202\n",
            "s\t1055\t0.00039\n",
            ".\t1012\t0.00116\n",
            "has\t2038\t0.00027\n",
            "evacuated\t13377\t0.00024\n",
            "and\t1998\t0.04673\n",
            "facilitated\t19601\t0.00070\n",
            "the\t1996\t0.89793\n",
            "evacuation\t13982\t0.00001\n",
            "of\t1997\t0.05382\n",
            "approximately\t3155\t0.01297\n",
            "95\t5345\t0.00022\n",
            ",\t1010\t0.07106\n",
            "700\t6352\t0.00002\n",
            "people\t2111\t0.00084\n",
            ".\t1012\t0.75792\n",
            "rt\t19387\t0.00026\n",
            "@\t1030\t0.00599\n",
            "white\t2317\t0.20159\n",
            "##house\t4580\t0.00000\n",
            ":\t1024\t0.04046\n",
            "update\t10651\t0.00127\n",
            ":\t1024\t0.04107\n",
            "from\t2013\t0.69520\n",
            "3\t1017\t0.01485\n",
            "am\t2572\t0.00386\n",
            "et\t3802\t0.00401\n",
            "on\t2006\t0.04731\n",
            "8\t1022\t0.05339\n",
            "/\t1013\t0.00993\n",
            "25\t2423\t0.00022\n",
            "to\t2000\t0.02577\n",
            "3a\t23842\t0.00202\n",
            "##m\t2213\t0.00087\n",
            "et\t3802\t0.00000\n",
            "on\t2006\t0.75098\n",
            "8\t1022\t0.00018\n",
            "/\t1013\t0.00003\n",
            "26\t2656\t0.00000\n",
            ",\t1010\t0.12994\n",
            "a\t1037\t0.99947\n",
            "total\t2561\t0.65830\n",
            "of\t1997\t0.01380\n",
            "approximately\t3155\t0.16008\n",
            "13\t2410\t0.00613\n",
            ",\t1010\t0.18443\n",
            "400\t4278\t0.00596\n",
            "people\t2111\t0.02840\n",
            "were\t2020\t0.00078\n",
            "evacuated\t13377\t0.00386\n",
            "from\t2013\t0.00076\n",
            "afghanistan\t7041\t0.00000\n",
            ".\t1012\t0.67045\n",
            "tune\t8694\t0.00694\n",
            "in\t1999\t0.00028\n",
            "as\t2004\t0.00353\n",
            "i\t1045\t0.00046\n",
            "discuss\t6848\t0.00004\n",
            "the\t1996\t0.75105\n",
            "whole\t2878\t0.00187\n",
            "-\t1011\t0.86552\n",
            "of\t1997\t0.00033\n",
            "-\t1011\t0.29641\n",
            "nation\t3842\t0.00003\n",
            "effort\t3947\t0.00001\n",
            "required\t3223\t0.00022\n",
            "to\t2000\t0.88961\n",
            "improve\t5335\t0.00005\n",
            "our\t2256\t0.00000\n",
            "cyber\t16941\t0.43717\n",
            "##se\t3366\t0.97331\n",
            "##cu\t10841\t0.00236\n",
            "##rity\t15780\t0.00001\n",
            ".\t1012\t0.84720\n",
            "rt\t19387\t0.00003\n",
            "@\t1030\t0.00009\n",
            "white\t2317\t0.02211\n",
            "##house\t4580\t0.00004\n",
            ":\t1024\t0.14411\n",
            "since\t2144\t0.07048\n",
            "august\t2257\t0.08200\n",
            "14\t2403\t0.00011\n",
            ",\t1010\t0.31010\n",
            "the\t1996\t0.96845\n",
            "u\t1057\t0.98781\n",
            ".\t1012\t0.64249\n",
            "s\t1055\t0.00035\n",
            ".\t1012\t0.01399\n",
            "has\t2038\t0.00200\n",
            "evacuated\t13377\t0.00030\n",
            "and\t1998\t0.02672\n",
            "facilitated\t19601\t0.00094\n",
            "the\t1996\t0.74483\n",
            "evacuation\t13982\t0.00120\n",
            "of\t1997\t0.04408\n",
            "approximately\t3155\t0.00584\n",
            "82\t6445\t0.00008\n",
            ",\t1010\t0.04176\n",
            "300\t3998\t0.00138\n",
            "people\t2111\t0.00010\n",
            "on\t2006\t0.00217\n",
            "us\t2149\t0.00277\n",
            "military\t2510\t0.00001\n",
            "and\t1998\t0.03871\n",
            "co\t2522\t0.00026\n",
            "…\t1529\t0.00000\n",
            "rt\t19387\t0.00009\n",
            "@\t1030\t0.00239\n",
            "white\t2317\t0.21661\n",
            "##house\t4580\t0.00000\n",
            ":\t1024\t0.04668\n",
            "update\t10651\t0.00104\n",
            ":\t1024\t0.08577\n",
            "from\t2013\t0.58519\n",
            "3\t1017\t0.06029\n",
            "am\t2572\t0.19935\n",
            "et\t3802\t0.00020\n",
            "on\t2006\t0.13965\n",
            "8\t1022\t0.00212\n",
            "/\t1013\t0.00010\n",
            "24\t2484\t0.00310\n",
            "to\t2000\t0.02979\n",
            "3\t1017\t0.02076\n",
            "am\t2572\t0.00043\n",
            "et\t3802\t0.00000\n",
            "on\t2006\t0.64306\n",
            "8\t1022\t0.00002\n",
            "/\t1013\t0.00001\n",
            "25\t2423\t0.00000\n",
            ",\t1010\t0.07072\n",
            "approximately\t3155\t0.08644\n",
            "19\t2539\t0.00229\n",
            ",\t1010\t0.99352\n",
            "000\t2199\t0.04174\n",
            "people\t2111\t0.02526\n",
            "were\t2020\t0.23875\n",
            "evacuated\t13377\t0.00047\n",
            "from\t2013\t0.00706\n",
            "kabul\t21073\t0.00000\n",
            ".\t1012\t0.67837\n",
            "today\t2651\t0.01406\n",
            ",\t1010\t0.89430\n",
            "the\t1996\t0.00705\n",
            "house\t2160\t0.00133\n",
            "took\t2165\t0.06799\n",
            "a\t1037\t0.98246\n",
            "significant\t3278\t0.01243\n",
            "step\t3357\t0.01578\n",
            "toward\t2646\t0.00222\n",
            "making\t2437\t0.00050\n",
            "historic\t3181\t0.00013\n",
            "investments\t10518\t0.00016\n",
            "that\t2008\t0.05107\n",
            "will\t2097\t0.00741\n",
            "transform\t10938\t0.00000\n",
            "america\t2637\t0.00125\n",
            "and\t1998\t0.11175\n",
            "cut\t3013\t0.00017\n",
            "taxes\t7773\t0.00000\n",
            "for\t2005\t0.15555\n",
            "working\t2551\t0.00023\n",
            "families\t2945\t0.00098\n",
            ".\t1012\t0.85749\n",
            "i\t1045\t0.59098\n",
            "want\t2215\t0.06495\n",
            "to\t2000\t0.07433\n",
            "thank\t4067\t0.00254\n",
            "everyone\t3071\t0.00012\n",
            "who\t2040\t0.00067\n",
            "voted\t5444\t0.00699\n",
            "today\t2651\t0.00001\n",
            "in\t1999\t0.09931\n",
            "support\t2490\t0.00233\n",
            "of\t1997\t0.34956\n",
            "the\t1996\t0.37692\n",
            "john\t2198\t0.03398\n",
            "lewis\t4572\t0.00000\n",
            "voting\t6830\t0.05017\n",
            "rights\t2916\t0.00697\n",
            "advancement\t12607\t0.00003\n",
            "act\t2552\t0.00000\n",
            ".\t1012\t0.86708\n",
            "we\t2057\t0.00831\n",
            "have\t2031\t0.01248\n",
            "helped\t3271\t0.00183\n",
            "to\t2000\t0.90052\n",
            "evacuate\t22811\t0.00004\n",
            "70\t3963\t0.00377\n",
            ",\t1010\t0.19364\n",
            "700\t6352\t0.00012\n",
            "people\t2111\t0.00002\n",
            "since\t2144\t0.00062\n",
            "august\t2257\t0.00248\n",
            "14\t2403\t0.00001\n",
            ".\t1012\t0.92896\n",
            "rt\t19387\t0.00001\n",
            "@\t1030\t0.00004\n",
            "white\t2317\t0.02148\n",
            "##house\t4580\t0.00001\n",
            ":\t1024\t0.41560\n",
            "since\t2144\t0.03390\n",
            "august\t2257\t0.11020\n",
            "14\t2403\t0.00000\n",
            ",\t1010\t0.05030\n",
            "the\t1996\t0.99813\n",
            "u\t1057\t0.99654\n",
            ".\t1012\t0.28505\n",
            "s\t1055\t0.00044\n",
            ".\t1012\t0.00111\n",
            "has\t2038\t0.00027\n",
            "evacuated\t13377\t0.00026\n",
            "and\t1998\t0.03901\n",
            "facilitated\t19601\t0.00073\n",
            "the\t1996\t0.89484\n",
            "evacuation\t13982\t0.00001\n",
            "of\t1997\t0.06518\n",
            "approximately\t3155\t0.00646\n",
            "58\t5388\t0.00068\n",
            ",\t1010\t0.07106\n",
            "700\t6352\t0.00002\n",
            "people\t2111\t0.00084\n",
            ".\t1012\t0.75792\n",
            "Genre: Wikipedia , mean perplexity: 7505.9604\n",
            "Genre: Yelp , mean perplexity: 231.28299\n",
            "Genre: Fiction , mean perplexity: 509125920.0\n",
            "Genre: Twitter , mean perplexity: 3991230.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question: \n",
        "What do you think are the reasons for the wide variation in perplexity of different categories of corpus? (hint: think about the training data of the pre-trained BERT model)\n",
        "\n",
        "\n",
        "Which of these is a true language model, and why?"
      ],
      "metadata": {
        "id": "TWk0jHUM8UFP"
      }
    }
  ]
}